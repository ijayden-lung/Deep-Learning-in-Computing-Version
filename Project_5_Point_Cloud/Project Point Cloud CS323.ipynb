{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this project, you will be asked to implement [PointNet](https://arxiv.org/abs/1612.00593) architecture and train a classification network (left) and a segmentation network (middle).\n",
    "![title](img/cls_sem.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Points\n",
    "* Task 1.1 - 5\n",
    "* Task 1.2 - 5\n",
    "* Task 2.1 - 10\n",
    "* Task 2.2 - 5\n",
    "* Task 2.3 - 5\n",
    "* Task 2.4 - 5\n",
    "* Task 2.5 - 5\n",
    "* Task 2.6 - 5\n",
    "* Task 2.7 - 5\n",
    "* Task 2.8 - 10\n",
    "* Task 2.9 - 10\n",
    "* Task 2.10 - 5 \n",
    "* Task 2.11 - 5\n",
    "* Task 2.12 - 5\n",
    "* Task 2.13 - 10\n",
    "* Task 2.14 - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is the simplest representation of 3D objects: only points in 3D space, no connectivity. Point clouds can also contain normals to points. \n",
    "here are three main constraints:\n",
    "\n",
    "    - Point clouds are unordered. Algorithm has to be invariant to permutations of the input set.\n",
    "    - If we rotate a chair, it’s still a chair. Network must be invariant to rigid transformations.\n",
    "    - Network should capture interactions among points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import dataset # custom dataset for ModelNet10 and ShapeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we write the point cloud as $X\\in\\mathbb{R}^{N\\times 3}$. While in programming, we use `B x 3 x N` layout, where `B` is the batch-size and `N` is the number of points in a single point cloud."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "objects can have different sizes and can be placed in different parts of our coordinate system.\n",
    " To augment the data during training, we randomly rotate objects around Z-axis and add Gaussian noise as described in the paper:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Jitter the position of each points by a zero mean Gaussian\n",
    "For input $X\\in\\mathbb{R}^{N\\times 3}$, we transform $X$ by $X \\leftarrow X + \\mathcal{N}(0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomJitter(object):\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        ## hint: useful function `torch.randn` and `torch.randn_like`\n",
    "        ## TASK 1.1\n",
    "        ## This function takes as input a point cloud of layout `3 x N`, \n",
    "        ## and output the jittered point cloud of layout `3 x N`.\n",
    "        \n",
    "        noise = torch.randn(data.shape)*self.sigma\n",
    "        data = data + noise\n",
    "      \n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1593)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random generate data and test your transform here\n",
    "randomJitter = RandomJitter(0.5)\n",
    "jitter_data = randomJitter.__call__(torch.randn(3,16))\n",
    "torch.std(jitter_data)   #std jitter_data = sqrt(var(data)+sigma^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Rotate the object along the z-axis randomly\n",
    "For input $X\\in\\mathbb{R}^{N\\times 3}$, we rotate all points along z-axis (up-axis) by a degree $\\theta$.\n",
    "\n",
    "\n",
    "Suppose $T$ is the transformation matrix,\n",
    "$$X\\leftarrow XT,$$\n",
    "where $$T=\\begin{bmatrix}\\cos\\theta & \\sin\\theta & 0 \\\\ -\\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomZRotate(object):\n",
    "    def __init__(self, degrees):\n",
    "        ## here `self.degrees` is a tuple (0, 360) which defines the range of degree\n",
    "        self.degrees = degrees\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        ## TASK 1.2\n",
    "        ## This function takes as input a point cloud of layout `3 x N`, \n",
    "        ## and output the rotated point cloud of layout `3 x N`.\n",
    "        ##\n",
    "        ## The rotation is along z-axis, and the degree is uniformly distributed\n",
    "        ## between [0, 360]\n",
    "        ##\n",
    "        ## hint: useful function `torch.randn`， `torch.randn_like` and `torch.matmul`\n",
    "        ##\n",
    "        ## Notice:   \n",
    "        ## Different from its math notation `N x 3`, the input has size of `3 x N`\n",
    "        \n",
    "        theta = random.random() * 2. * math.pi \n",
    "        T = torch.Tensor([[np.cos(theta), np.sin(theta),0],\n",
    "                         [-np.sin(theta),np.cos(theta),0],\n",
    "                         [0,0,1]])\n",
    "        data = torch.matmul(T,data)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2353,  0.7301, -0.4874,  0.0224,  0.6520,  0.4408, -1.4210, -0.6609,\n",
      "         -0.7314,  0.8657,  2.4664, -0.6494, -2.6774, -0.0770, -1.2609, -0.2580],\n",
      "        [-0.4670,  0.6016, -1.4237,  0.3318,  2.6455,  1.3588, -1.0875,  0.4159,\n",
      "          0.8073, -1.1750, -0.1032,  1.9778,  0.5814, -2.0564, -0.5687, -0.5860],\n",
      "        [ 1.5230,  0.5756, -0.7889, -1.1402, -0.2148, -0.5296, -0.7789, -0.2542,\n",
      "         -1.4677,  0.3994, -0.2039, -0.5506,  0.4056, -1.7912,  0.2908, -1.6654]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4754, -0.9459,  1.2697, -0.2247, -2.1610, -1.1927,  1.7890,  0.2566,\n",
       "          0.0672,  0.0574, -1.8622, -0.7276,  1.7285,  1.3441,  1.3400,  0.5674],\n",
       "        [ 0.2179, -0.0141,  0.8078, -0.2452, -1.6594, -0.7862, -0.0377, -0.7375,\n",
       "         -1.0873,  1.4583,  1.6205, -1.9504, -2.1258,  1.5582, -0.3430,  0.2967],\n",
       "        [ 1.5230,  0.5756, -0.7889, -1.1402, -0.2148, -0.5296, -0.7789, -0.2542,\n",
       "         -1.4677,  0.3994, -0.2039, -0.5506,  0.4056, -1.7912,  0.2908, -1.6654]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random generate data and test your transform here\n",
    "data_ = torch.randn(3,16)\n",
    "print(data_)\n",
    "randomZRotate = RandomZRotate((0,360))\n",
    "randomZRotate.__call__(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load dataset ModelNet10 for Point Cloud Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelNet10\n",
    "By loading this dataset, we have data of shape `B x 3 x N` and label of shape `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may taske some time to download and pre-process the dataset.\n",
    "train_transform = Compose([RandomZRotate((0, 360)), RandomJitter(0.02)])\n",
    "train_cls_dataset = dataset.ModelNet(root='./ModelNet10', transform=train_transform, train=True)\n",
    "test_cls_dataset = dataset.ModelNet(root='./ModelNet10', train=False)\n",
    "train_cls_loader = data.DataLoader(\n",
    "    train_cls_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "test_cls_loader = data.DataLoader(\n",
    "    test_cls_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(train_cls_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShapeNet\n",
    "By loading this dataset, we have data of shape `B x 3 x N` and target of shape `B x N`.\n",
    "\n",
    "Here is the list of categories:\n",
    "['Airplane', 'Bag', 'Cap', 'Car', 'Chair', 'Earphone', 'Guitar', 'Knife', 'Lamp', 'Laptop', 'Motorbike', 'Mug', 'Pistol', 'Rocket', 'Skateboard', 'Table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/train_data.zip\n",
      "Extracting ./ShapeNet/raw/train_data.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/train_label.zip\n",
      "Extracting ./ShapeNet/raw/train_label.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/val_data.zip\n",
      "Extracting ./ShapeNet/raw/val_data.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/val_label.zip\n",
      "Extracting ./ShapeNet/raw/val_label.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/test_data.zip\n",
      "Extracting ./ShapeNet/raw/test_data.zip\n",
      "Downloading https://shapenet.cs.stanford.edu/iccv17/partseg/test_label.zip\n",
      "Extracting ./ShapeNet/raw/test_label.zip\n",
      "Done!\n",
      "Processing...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "## Here as an example, we choose the cateogry 'Airplane'\n",
    "category = 'Airplane'\n",
    "train_seg_dataset = dataset.ShapeNet(root='./ShapeNet', category=category, train=True)\n",
    "test_seg_dataset = dataset.ShapeNet(root='./ShapeNet', category=category, train=False)\n",
    "train_seg_loader = data.DataLoader(\n",
    "    train_seg_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "test_seg_loader = data.DataLoader(\n",
    "    test_seg_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(train_seg_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 PointNet Architecture (Read PointNet Section 4.2 and Appendix C)\n",
    "In this section, you will be asked to implement classification and segmentation step by step.\n",
    "![pointnet](img/pointnet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Joint Alignment Network \n",
    "This mini-network takes as input matrix of size $N \\times K$, and outputs a transformation matrix of size $K \\times K$. \n",
    "\n",
    "In programming, the input size of this module is `B x K x N` and output size is `B x K x K`.\n",
    "\n",
    "For the shared MLP, use structure like this `(FC(64), BN, ReLU, FC(128), BN, ReLU, FC(1024), BN, ReLU)`.\n",
    "\n",
    "For the MLP after global max pooling, use structure like this `(FC(512), BN, ReLU, FC(256), BN, ReLU, FC(K*K)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ensure invariance to transformations\n",
    "class Transformation(nn.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super(Transformation, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        \n",
    "        ## TASK 2.1\n",
    "        \n",
    "        ## define your network layers here\n",
    "        ## shared mlp\n",
    "        ## input size: B x K x N\n",
    "        ## output size: B x 1024 x N\n",
    "        ## hint: you may want to use `nn.Conv1d` here. Why?\n",
    "        ## First of all, our tensors will have size (B, K, 3). \n",
    "        ## In this case MLP with shared weights is just 1-dim convolution with a kernel of size 1.\n",
    "        self.share_mlp = nn.Sequential(nn.Conv1d(k, 64, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv1d(64, 128,1, stride=1),\n",
    "                                      nn.BatchNorm1d(128),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv1d(128, 1024, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(1024),\n",
    "                                      nn.ReLU())\n",
    "\n",
    "        ## define your network layers here\n",
    "        ## mlp\n",
    "        ## input size: B x 1024\n",
    "        ## output size: B x (K*K)\n",
    "        \n",
    "        self.mlp = nn.Sequential(nn.Linear(1024, 512), \n",
    "                                 nn.BatchNorm1d(512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(512, 256),\n",
    "                                 # batch size should be larger than 1, otherwise there will have an error\n",
    "                                 nn.BatchNorm1d(256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(256, k*k))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, K, N = x.shape # batch-size, dim, number of points\n",
    "        ## TASK 2.1\n",
    "\n",
    "        ## forward of shared mlp\n",
    "        # input - B x K x N\n",
    "        # output - B x 1024 x N\n",
    "        x = self.share_mlp(x)\n",
    "        \n",
    "        ## global max pooling\n",
    "        # input - B x 1024 x N\n",
    "        # output - B x 1024\n",
    "        #To provide permutation invariance, we apply a symmetric function (max pooling) to the extracted\n",
    "        #and transformed features so the result does not depend on the order of input points anymore.\n",
    "        x = nn.MaxPool1d(N)(x)\n",
    "        x = x.view(-1,1024)\n",
    "        \n",
    "        ## mlp\n",
    "        # input - B x 1024\n",
    "        # output - B x (K*K)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        ## reshape the transformation matrix to B x K x K\n",
    "        identity = torch.eye(self.k, device=x.device)\n",
    "        x = x.view(B, self.k, self.k) + identity[None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random generate data and test this network\n",
    "transformation = Transformation()\n",
    "transformation(torch.randn(5,3,8)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Regularization Loss\n",
    "$$L_{reg}=\\|I-TT^\\intercal\\|^2_F$$\n",
    "\n",
    "The output of `Transformation` network is of size `B x K x K`. The module `OrthoLoss` has no trainable parameters, only computes this norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthoLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrthoLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## hint: useful function `torch.bmm` and `torch.matmul`\n",
    "        \n",
    "        ## TASK 2.2\n",
    "        ## compute the matrix product\n",
    "        prod = torch.bmm(x,torch.transpose(x, 1, 2))\n",
    "\n",
    "        norm = torch.norm(prod - torch.eye(x.shape[1], device=x.device)[None], dim=(1,2))\n",
    "        return norm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1356)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random generate data and test this module\n",
    "orthoLoss = OrthoLoss()\n",
    "orthoLoss(torch.randn(5,3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Network\n",
    "In this subsection, you will be asked to implement the feature network (the top branch).\n",
    "\n",
    "Local features are a matrix of size `B x 64 x N`, which will be used in the segmentation task.\n",
    "\n",
    "Global features are a matrix of size `B x 1024`, which will be used in the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature(nn.Module):\n",
    "    def __init__(self, alignment=False):\n",
    "        super(Feature, self).__init__()\n",
    "        \n",
    "        self.alignment = alignment\n",
    "        \n",
    "        ## `input_transform` calculates the input transform matrix of size `3 x 3`\n",
    "        if self.alignment:\n",
    "            self.input_transform = Transformation(3)\n",
    "        \n",
    "        ## TASK 2.3\n",
    "        ## define your network layers here\n",
    "        ## local feature\n",
    "        ## shared mlp\n",
    "        ## input size: B x 3 x N\n",
    "        ## output size: B x 64 x N\n",
    "        ## hint: you may want to use `nn.Conv1d` here.\n",
    "        self.local_feature = nn.Sequential(nn.Conv1d(3, 64, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv1d(64, 64, 1, stride=1),\n",
    "                                      nn.BatchNorm1d(64),\n",
    "                                      nn.ReLU())\n",
    "        \n",
    "        ## `feature_transform` calculates the feature transform matrix of size `64 x 64`\n",
    "        if self.alignment:\n",
    "            self.feature_transform = Transformation(64)\n",
    "        \n",
    "        ## TASK 2.4\n",
    "        ## define your network layers here\n",
    "        ## global feature\n",
    "        ## shared mlp\n",
    "        ## input size: B x 64 x N\n",
    "        ## output size: B x 1024 x N      \n",
    "        self.global_feature = nn.Sequential(\n",
    "                              nn.Conv1d(64, 128,1, stride=1),\n",
    "                              nn.BatchNorm1d(128),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv1d(128, 1024, 1, stride=1),\n",
    "                              nn.BatchNorm1d(1024),\n",
    "                              nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ## apply the input transform\n",
    "        if self.alignment:\n",
    "            transform = self.input_transform(x)\n",
    "            ## TASK 2.5\n",
    "            ## apply the input transform\n",
    "            x = torch.bmm(transform,x)\n",
    "\n",
    "        ## TASK 2.3\n",
    "        ## forward of shared mlp\n",
    "        # input - B x K x N\n",
    "        # output - B x 64 x N\n",
    "        x = self.local_feature(x)\n",
    "        \n",
    "        if self.alignment:\n",
    "            transform = self.feature_transform(x)\n",
    "            ## TASK 2.5\n",
    "            ## apply the feature transform\n",
    "            x = torch.bmm(transform,x)\n",
    "        else:\n",
    "            ## do not modify this line\n",
    "            transform = None\n",
    "        \n",
    "        local_feature = x\n",
    "        \n",
    "        ## TASK 2.4\n",
    "        ## forward of shared mlp\n",
    "        # input - B x 64 x N\n",
    "        # output - B x 1024 x N\n",
    "        x = self.global_feature(x)\n",
    "        \n",
    "        ## TASK 2.4\n",
    "        ## global max pooling\n",
    "        # input - B x 1024 x N\n",
    "        # output - B x 1024\n",
    "        x = nn.MaxPool1d(x.shape[2])(x)\n",
    "        x = x.view(-1,1024)\n",
    "        \n",
    "        global_feature = x\n",
    "        \n",
    "        ## summary:\n",
    "        ## global_feature: B x 1024\n",
    "        ## local_feature: B x 64 x N\n",
    "        ## transform: B x K x K\n",
    "        return global_feature, local_feature, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1024]), torch.Size([3, 64, 5]), torch.Size([3, 64, 64]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random generate data and test this network\n",
    "feature = Feature(True)\n",
    "global_feature,local_feature,transform = feature(torch.randn(3,3,5))\n",
    "global_feature.shape,local_feature.shape,transform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Classification Network\n",
    "In this network, you will use the global features generated by the `Feature` network defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, num_classes, alignment=False):\n",
    "        super(Classification, self).__init__()\n",
    "                \n",
    "        self.feature = Feature(alignment=alignment)\n",
    "        \n",
    "        ## TASK 2.6\n",
    "        ## define your network layers here\n",
    "        ## mlp\n",
    "        ## input size: B x 1024\n",
    "        ## output size: B x num_classes\n",
    "        self.mlp = nn.Sequential(nn.Linear(1024, 512),\n",
    "                         nn.BatchNorm1d(512),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(512, 256),\n",
    "                         nn.Dropout(0.3),\n",
    "                         nn.BatchNorm1d(256),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(256, 128),\n",
    "                         nn.BatchNorm1d(128),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(128, num_classes))\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is the global feature matrix\n",
    "        # here we don't use local feature matrix\n",
    "        x, _, trans = self.feature(x)\n",
    "        \n",
    "        ## TASK 2.6\n",
    "        ## forward of mlp\n",
    "        # input - B x 1024\n",
    "        # output - B x num_classes        \n",
    "        x = self.mlp(x)\n",
    "        x = self.logsoftmax(x)\n",
    "        #The log-softmax penalty has a exponential nature \n",
    "        #compared to the linear penalisation of softmax. \n",
    "        #i.e More heavy peanlty for being more wrong.  \n",
    "        ## x: B x num_classes\n",
    "        ## trans: B x K x K\n",
    "        return x, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10]), torch.Size([3, 64, 64]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random generate data and test this network\n",
    "classification = Classification(10,alignment=True)\n",
    "output,trans = classification(torch.randn(3,3,5))\n",
    "output.shape,trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Train this network on ModelNet10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main train function for classification\n",
    "def train_cls(train_loader, test_loader, network, optimizer, epochs, scheduler):\n",
    "    reg = OrthoLoss()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch:[{:02d}/{:02d}]'.format(epoch+1, epochs))\n",
    "        print('Training...')\n",
    "        network.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        for batch, (pos, label) in enumerate(train_loader):\n",
    "            network.zero_grad()\n",
    "            pos, label = pos.cuda(), label.cuda()\n",
    "            \n",
    "            ## TASK 2.7\n",
    "            ## forward propagation\n",
    "            output, trans = network(pos)\n",
    "            #As we used LogSoftmax for stability, \n",
    "            #we should apply NLLLoss instead of CrossEntropyLoss\n",
    "            loss = nn.NLLLoss()(output,label) \n",
    "            ##########\n",
    "            \n",
    "            ## regularizer\n",
    "            if trans is not None:\n",
    "                loss += reg(trans) * 0.001\n",
    "\n",
    "            pred = output.max(1)[1]\n",
    "            correct += pred.eq(label).sum().item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(train_loader), loss.item()), end='', flush=True)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}'.format(train_loss/len(train_loader), correct/len(train_loader.dataset) * 100))\n",
    "        \n",
    "        print('\\nTesting...')\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            for batch, (pos, label) in enumerate(test_loader):\n",
    "                pos, label = pos.cuda(), label.cuda()\n",
    "    \n",
    "                ## TASK 2.7\n",
    "                ## forward propagation\n",
    "                output, trans = network(pos)\n",
    "                loss = nn.NLLLoss()(output,label) \n",
    "                ##########\n",
    "\n",
    "                if trans is not None:\n",
    "                    loss += reg(trans) * 0.001\n",
    "\n",
    "                pred = output.max(1)[1]\n",
    "                correct += pred.eq(label).sum().item()\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(test_loader), loss.item()), end='', flush=True)\n",
    "\n",
    "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}'.format(test_loss/len(test_loader), correct/len(test_loader.dataset) * 100))\n",
    "        print('-------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[01/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.5615\n",
      "Average Train Loss: 1.5048; Train Acc: 49.5866\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 3.2281\n",
      "Average Test Loss: 1.4132; Test Acc: 48.0176\n",
      "-------------------------------------------\n",
      "Epoch:[02/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 2.4022\n",
      "Average Train Loss: 1.0857; Train Acc: 64.4701\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.4890\n",
      "Average Test Loss: 1.1109; Test Acc: 62.0044\n",
      "-------------------------------------------\n",
      "Epoch:[03/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 2.1093\n",
      "Average Train Loss: 0.9585; Train Acc: 68.3538\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 2.2361\n",
      "Average Test Loss: 1.0871; Test Acc: 57.5991\n",
      "-------------------------------------------\n",
      "Epoch:[04/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.9409\n",
      "Average Train Loss: 0.8604; Train Acc: 72.3127\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 2.3093\n",
      "Average Test Loss: 0.8047; Test Acc: 70.8150\n",
      "-------------------------------------------\n",
      "Epoch:[05/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 2.9482\n",
      "Average Train Loss: 0.7741; Train Acc: 74.2922\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 1.4719\n",
      "Average Test Loss: 0.7001; Test Acc: 75.4405\n",
      "-------------------------------------------\n",
      "Epoch:[06/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 2.2283\n",
      "Average Train Loss: 0.7079; Train Acc: 76.2967\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 3.0990\n",
      "Average Test Loss: 0.7165; Test Acc: 77.3128\n",
      "-------------------------------------------\n",
      "Epoch:[07/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4833\n",
      "Average Train Loss: 0.6569; Train Acc: 78.3763\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.4761\n",
      "Average Test Loss: 0.7179; Test Acc: 72.2467\n",
      "-------------------------------------------\n",
      "Epoch:[08/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9333\n",
      "Average Train Loss: 0.6121; Train Acc: 79.7544\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.8352\n",
      "Average Test Loss: 0.7542; Test Acc: 74.7797\n",
      "-------------------------------------------\n",
      "Epoch:[09/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2179\n",
      "Average Train Loss: 0.5793; Train Acc: 81.2829\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.3860\n",
      "Average Test Loss: 0.5462; Test Acc: 81.4978\n",
      "-------------------------------------------\n",
      "Epoch:[10/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 3.0056\n",
      "Average Train Loss: 0.5844; Train Acc: 81.0323\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.6649\n",
      "Average Test Loss: 0.5630; Test Acc: 81.9383\n",
      "-------------------------------------------\n",
      "Epoch:[11/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.0709\n",
      "Average Train Loss: 0.5307; Train Acc: 82.7362\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 1.23190\n",
      "Average Test Loss: 0.6000; Test Acc: 78.4141\n",
      "-------------------------------------------\n",
      "Epoch:[12/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4996\n",
      "Average Train Loss: 0.5242; Train Acc: 82.3102\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 2.3505\n",
      "Average Test Loss: 0.6149; Test Acc: 80.3965\n",
      "-------------------------------------------\n",
      "Epoch:[13/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.2047\n",
      "Average Train Loss: 0.5179; Train Acc: 82.8113\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 1.0092\n",
      "Average Test Loss: 0.6347; Test Acc: 76.1013\n",
      "-------------------------------------------\n",
      "Epoch:[14/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4181\n",
      "Average Train Loss: 0.4967; Train Acc: 84.0641\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0352\n",
      "Average Test Loss: 0.5730; Test Acc: 80.1762\n",
      "-------------------------------------------\n",
      "Epoch:[15/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.1633\n",
      "Average Train Loss: 0.4855; Train Acc: 83.9389\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0635\n",
      "Average Test Loss: 0.7416; Test Acc: 71.2555\n",
      "-------------------------------------------\n",
      "Epoch:[16/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4412\n",
      "Average Train Loss: 0.4729; Train Acc: 84.0391\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0959\n",
      "Average Test Loss: 0.5053; Test Acc: 83.2599\n",
      "-------------------------------------------\n",
      "Epoch:[17/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0285\n",
      "Average Train Loss: 0.4707; Train Acc: 83.8136\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 1.9706\n",
      "Average Test Loss: 0.5872; Test Acc: 78.3040\n",
      "-------------------------------------------\n",
      "Epoch:[18/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1557\n",
      "Average Train Loss: 0.4515; Train Acc: 85.2669\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.5978\n",
      "Average Test Loss: 0.5827; Test Acc: 79.1850\n",
      "-------------------------------------------\n",
      "Epoch:[19/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4123\n",
      "Average Train Loss: 0.4564; Train Acc: 84.8659\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.9745\n",
      "Average Test Loss: 0.6387; Test Acc: 77.6432\n",
      "-------------------------------------------\n",
      "Epoch:[20/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2990\n",
      "Average Train Loss: 0.4061; Train Acc: 86.4194\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0459\n",
      "Average Test Loss: 0.4644; Test Acc: 84.8018\n",
      "-------------------------------------------\n",
      "Epoch:[21/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.1117\n",
      "Average Train Loss: 0.3453; Train Acc: 88.4741\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0959\n",
      "Average Test Loss: 0.3976; Test Acc: 86.0132\n",
      "-------------------------------------------\n",
      "Epoch:[22/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.2480\n",
      "Average Train Loss: 0.3249; Train Acc: 89.1005\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 1.00594\n",
      "Average Test Loss: 0.4310; Test Acc: 86.4537\n",
      "-------------------------------------------\n",
      "Epoch:[23/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4722\n",
      "Average Train Loss: 0.3327; Train Acc: 89.0754\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.3072\n",
      "Average Test Loss: 0.4512; Test Acc: 84.9119\n",
      "-------------------------------------------\n",
      "Epoch:[24/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0285\n",
      "Average Train Loss: 0.3243; Train Acc: 90.0025\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.08255\n",
      "Average Test Loss: 0.4590; Test Acc: 84.6916\n",
      "-------------------------------------------\n",
      "Epoch:[25/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3146\n",
      "Average Train Loss: 0.3133; Train Acc: 89.6768\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0596\n",
      "Average Test Loss: 0.3928; Test Acc: 86.8943\n",
      "-------------------------------------------\n",
      "Epoch:[26/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1418\n",
      "Average Train Loss: 0.3321; Train Acc: 88.6996\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.09008\n",
      "Average Test Loss: 0.4527; Test Acc: 83.8106\n",
      "-------------------------------------------\n",
      "Epoch:[27/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.5914\n",
      "Average Train Loss: 0.3105; Train Acc: 89.8271\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.3613\n",
      "Average Test Loss: 0.4241; Test Acc: 84.4714\n",
      "-------------------------------------------\n",
      "Epoch:[28/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7010\n",
      "Average Train Loss: 0.2921; Train Acc: 90.2531\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.1797\n",
      "Average Test Loss: 0.4692; Test Acc: 82.5991\n",
      "-------------------------------------------\n",
      "Epoch:[29/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.0773\n",
      "Average Train Loss: 0.3014; Train Acc: 89.9524\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.1495\n",
      "Average Test Loss: 0.4175; Test Acc: 86.7841\n",
      "-------------------------------------------\n",
      "Epoch:[30/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1077\n",
      "Average Train Loss: 0.3134; Train Acc: 89.6768\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.22950\n",
      "Average Test Loss: 0.4600; Test Acc: 84.6916\n",
      "-------------------------------------------\n",
      "Epoch:[31/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4750\n",
      "Average Train Loss: 0.3172; Train Acc: 88.8499\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0827\n",
      "Average Test Loss: 0.4382; Test Acc: 85.4626\n",
      "-------------------------------------------\n",
      "Epoch:[32/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3928\n",
      "Average Train Loss: 0.2789; Train Acc: 90.5287\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 1.60518\n",
      "Average Test Loss: 0.4103; Test Acc: 86.5639\n",
      "-------------------------------------------\n",
      "Epoch:[33/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1236\n",
      "Average Train Loss: 0.2700; Train Acc: 91.2303\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.1348\n",
      "Average Test Loss: 0.3906; Test Acc: 87.4449\n",
      "-------------------------------------------\n",
      "Epoch:[34/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9441\n",
      "Average Train Loss: 0.2683; Train Acc: 90.9296\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.02272\n",
      "Average Test Loss: 0.4209; Test Acc: 84.8018\n",
      "-------------------------------------------\n",
      "Epoch:[35/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7971\n",
      "Average Train Loss: 0.2813; Train Acc: 91.0298\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0087\n",
      "Average Test Loss: 0.5336; Test Acc: 80.8370\n",
      "-------------------------------------------\n",
      "Epoch:[36/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3882\n",
      "Average Train Loss: 0.2471; Train Acc: 91.7815\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.34135\n",
      "Average Test Loss: 0.3845; Test Acc: 88.2159\n",
      "-------------------------------------------\n",
      "Epoch:[37/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2814\n",
      "Average Train Loss: 0.2822; Train Acc: 90.8544\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.20583\n",
      "Average Test Loss: 0.4216; Test Acc: 86.7841\n",
      "-------------------------------------------\n",
      "Epoch:[38/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0469\n",
      "Average Train Loss: 0.2533; Train Acc: 91.4307\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.03162\n",
      "Average Test Loss: 0.4424; Test Acc: 85.0220\n",
      "-------------------------------------------\n",
      "Epoch:[39/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7147\n",
      "Average Train Loss: 0.2742; Train Acc: 90.9296\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.1642\n",
      "Average Test Loss: 0.4725; Test Acc: 85.3524\n",
      "-------------------------------------------\n",
      "Epoch:[40/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0252\n",
      "Average Train Loss: 0.2564; Train Acc: 91.7565\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00860\n",
      "Average Test Loss: 0.3757; Test Acc: 88.7665\n",
      "-------------------------------------------\n",
      "Epoch:[41/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2985\n",
      "Average Train Loss: 0.2130; Train Acc: 92.5332\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.03687\n",
      "Average Test Loss: 0.3863; Test Acc: 88.3260\n",
      "-------------------------------------------\n",
      "Epoch:[42/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.5173\n",
      "Average Train Loss: 0.2166; Train Acc: 92.6334\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00763\n",
      "Average Test Loss: 0.3993; Test Acc: 86.6740\n",
      "-------------------------------------------\n",
      "Epoch:[43/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1986\n",
      "Average Train Loss: 0.2161; Train Acc: 92.9842\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0304\n",
      "Average Test Loss: 0.3331; Test Acc: 89.4273\n",
      "-------------------------------------------\n",
      "Epoch:[44/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4174\n",
      "Average Train Loss: 0.2079; Train Acc: 93.2849\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.03401\n",
      "Average Test Loss: 0.3538; Test Acc: 88.3260\n",
      "-------------------------------------------\n",
      "Epoch:[45/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6777\n",
      "Average Train Loss: 0.2037; Train Acc: 93.1346\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.06566\n",
      "Average Test Loss: 0.3900; Test Acc: 87.4449\n",
      "-------------------------------------------\n",
      "Epoch:[46/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0330\n",
      "Average Train Loss: 0.2045; Train Acc: 92.7086\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.08784\n",
      "Average Test Loss: 0.3405; Test Acc: 88.4361\n",
      "-------------------------------------------\n",
      "Epoch:[47/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1890\n",
      "Average Train Loss: 0.1834; Train Acc: 93.7359\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0486\n",
      "Average Test Loss: 0.3524; Test Acc: 89.3172\n",
      "-------------------------------------------\n",
      "Epoch:[48/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.1555\n",
      "Average Train Loss: 0.1910; Train Acc: 93.5605\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.09627\n",
      "Average Test Loss: 0.3903; Test Acc: 87.9956\n",
      "-------------------------------------------\n",
      "Epoch:[49/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7989\n",
      "Average Train Loss: 0.1985; Train Acc: 93.0093\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.26295\n",
      "Average Test Loss: 0.3823; Test Acc: 86.6740\n",
      "-------------------------------------------\n",
      "Epoch:[50/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.0835\n",
      "Average Train Loss: 0.1932; Train Acc: 93.6106\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.18300\n",
      "Average Test Loss: 0.3756; Test Acc: 87.9956\n",
      "-------------------------------------------\n",
      "Epoch:[51/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.5691\n",
      "Average Train Loss: 0.2074; Train Acc: 93.2849\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.0440\n",
      "Average Test Loss: 0.3623; Test Acc: 88.8767\n",
      "-------------------------------------------\n",
      "Epoch:[52/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0570\n",
      "Average Train Loss: 0.1923; Train Acc: 93.7359\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.05259\n",
      "Average Test Loss: 0.3574; Test Acc: 89.3172\n",
      "-------------------------------------------\n",
      "Epoch:[53/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7279\n",
      "Average Train Loss: 0.1709; Train Acc: 94.1118\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01528\n",
      "Average Test Loss: 0.3677; Test Acc: 88.9868\n",
      "-------------------------------------------\n",
      "Epoch:[54/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7597\n",
      "Average Train Loss: 0.1851; Train Acc: 93.8862\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00966\n",
      "Average Test Loss: 0.3412; Test Acc: 88.8767\n",
      "-------------------------------------------\n",
      "Epoch:[55/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0062\n",
      "Average Train Loss: 0.1698; Train Acc: 94.2120\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01333\n",
      "Average Test Loss: 0.4416; Test Acc: 83.9207\n",
      "-------------------------------------------\n",
      "Epoch:[56/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0356\n",
      "Average Train Loss: 0.1859; Train Acc: 94.1869\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.03545\n",
      "Average Test Loss: 0.3666; Test Acc: 89.7577\n",
      "-------------------------------------------\n",
      "Epoch:[57/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0334\n",
      "Average Train Loss: 0.1764; Train Acc: 93.9865\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01052\n",
      "Average Test Loss: 0.3570; Test Acc: 88.8767\n",
      "-------------------------------------------\n",
      "Epoch:[58/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9201\n",
      "Average Train Loss: 0.1785; Train Acc: 94.1619\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.02922\n",
      "Average Test Loss: 0.3532; Test Acc: 89.8678\n",
      "-------------------------------------------\n",
      "Epoch:[59/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1263\n",
      "Average Train Loss: 0.1910; Train Acc: 93.4853\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01932\n",
      "Average Test Loss: 0.3491; Test Acc: 89.9780\n",
      "-------------------------------------------\n",
      "Epoch:[60/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.9104\n",
      "Average Train Loss: 0.1716; Train Acc: 94.6129\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00679\n",
      "Average Test Loss: 0.4138; Test Acc: 86.8943\n",
      "-------------------------------------------\n",
      "Epoch:[61/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4087\n",
      "Average Train Loss: 0.1559; Train Acc: 94.8384\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00503\n",
      "Average Test Loss: 0.3376; Test Acc: 90.0881\n",
      "-------------------------------------------\n",
      "Epoch:[62/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.4369\n",
      "Average Train Loss: 0.1475; Train Acc: 95.0138\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00419\n",
      "Average Test Loss: 0.3789; Test Acc: 87.8855\n",
      "-------------------------------------------\n",
      "Epoch:[63/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 5.1321\n",
      "Average Train Loss: 0.1615; Train Acc: 95.1391\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00637\n",
      "Average Test Loss: 0.3555; Test Acc: 88.4361\n",
      "-------------------------------------------\n",
      "Epoch:[64/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0949\n",
      "Average Train Loss: 0.1482; Train Acc: 95.0138\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00497\n",
      "Average Test Loss: 0.3477; Test Acc: 89.5374\n",
      "-------------------------------------------\n",
      "Epoch:[65/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0028\n",
      "Average Train Loss: 0.1556; Train Acc: 94.4375\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00707\n",
      "Average Test Loss: 0.3190; Test Acc: 90.5286\n",
      "-------------------------------------------\n",
      "Epoch:[66/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6138\n",
      "Average Train Loss: 0.1423; Train Acc: 95.1892\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00407\n",
      "Average Test Loss: 0.3497; Test Acc: 88.6564\n",
      "-------------------------------------------\n",
      "Epoch:[67/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0690\n",
      "Average Train Loss: 0.1445; Train Acc: 94.7632\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00798\n",
      "Average Test Loss: 0.3482; Test Acc: 89.6476\n",
      "-------------------------------------------\n",
      "Epoch:[68/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.3282\n",
      "Average Train Loss: 0.1349; Train Acc: 95.5149\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00351\n",
      "Average Test Loss: 0.3549; Test Acc: 88.5463\n",
      "-------------------------------------------\n",
      "Epoch:[69/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0091\n",
      "Average Train Loss: 0.1380; Train Acc: 95.3896\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00836\n",
      "Average Test Loss: 0.3684; Test Acc: 89.2070\n",
      "-------------------------------------------\n",
      "Epoch:[70/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0377\n",
      "Average Train Loss: 0.1525; Train Acc: 94.6880\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.04960\n",
      "Average Test Loss: 0.3578; Test Acc: 88.5463\n",
      "-------------------------------------------\n",
      "Epoch:[71/100]\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: [250/250] Loss: 0.0762\n",
      "Average Train Loss: 0.1498; Train Acc: 94.9136\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01119\n",
      "Average Test Loss: 0.3779; Test Acc: 88.4361\n",
      "-------------------------------------------\n",
      "Epoch:[72/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3298\n",
      "Average Train Loss: 0.1319; Train Acc: 95.2643\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01324\n",
      "Average Test Loss: 0.3792; Test Acc: 87.4449\n",
      "-------------------------------------------\n",
      "Epoch:[73/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0712\n",
      "Average Train Loss: 0.1386; Train Acc: 95.4397\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01663\n",
      "Average Test Loss: 0.3601; Test Acc: 88.9868\n",
      "-------------------------------------------\n",
      "Epoch:[74/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 2.0074\n",
      "Average Train Loss: 0.1433; Train Acc: 95.2894\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.03186\n",
      "Average Test Loss: 0.3425; Test Acc: 89.9780\n",
      "-------------------------------------------\n",
      "Epoch:[75/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0852\n",
      "Average Train Loss: 0.1349; Train Acc: 95.2643\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00152\n",
      "Average Test Loss: 0.3822; Test Acc: 88.1057\n",
      "-------------------------------------------\n",
      "Epoch:[76/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.7512\n",
      "Average Train Loss: 0.1341; Train Acc: 95.4899\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01332\n",
      "Average Test Loss: 0.3582; Test Acc: 87.7753\n",
      "-------------------------------------------\n",
      "Epoch:[77/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0692\n",
      "Average Train Loss: 0.1425; Train Acc: 95.0138\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00717\n",
      "Average Test Loss: 0.3585; Test Acc: 88.8767\n",
      "-------------------------------------------\n",
      "Epoch:[78/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0234\n",
      "Average Train Loss: 0.1351; Train Acc: 95.3646\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00298\n",
      "Average Test Loss: 0.3455; Test Acc: 90.0881\n",
      "-------------------------------------------\n",
      "Epoch:[79/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.5049\n",
      "Average Train Loss: 0.1386; Train Acc: 95.3896\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00615\n",
      "Average Test Loss: 0.3913; Test Acc: 88.6564\n",
      "-------------------------------------------\n",
      "Epoch:[80/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.6674\n",
      "Average Train Loss: 0.1404; Train Acc: 95.1641\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00227\n",
      "Average Test Loss: 0.3376; Test Acc: 89.2070\n",
      "-------------------------------------------\n",
      "Epoch:[81/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3702\n",
      "Average Train Loss: 0.1301; Train Acc: 95.2643\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00344\n",
      "Average Test Loss: 0.3425; Test Acc: 90.1982\n",
      "-------------------------------------------\n",
      "Epoch:[82/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1376\n",
      "Average Train Loss: 0.1205; Train Acc: 95.8156\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00457\n",
      "Average Test Loss: 0.3442; Test Acc: 90.3084\n",
      "-------------------------------------------\n",
      "Epoch:[83/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3830\n",
      "Average Train Loss: 0.1098; Train Acc: 95.8657\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00349\n",
      "Average Test Loss: 0.3351; Test Acc: 90.1982\n",
      "-------------------------------------------\n",
      "Epoch:[84/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0035\n",
      "Average Train Loss: 0.1177; Train Acc: 95.9158\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00286\n",
      "Average Test Loss: 0.3529; Test Acc: 88.9868\n",
      "-------------------------------------------\n",
      "Epoch:[85/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 1.2419\n",
      "Average Train Loss: 0.1279; Train Acc: 95.6402\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00676\n",
      "Average Test Loss: 0.3467; Test Acc: 90.4185\n",
      "-------------------------------------------\n",
      "Epoch:[86/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3170\n",
      "Average Train Loss: 0.1260; Train Acc: 95.7404\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00699\n",
      "Average Test Loss: 0.3488; Test Acc: 90.6388\n",
      "-------------------------------------------\n",
      "Epoch:[87/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6657\n",
      "Average Train Loss: 0.1283; Train Acc: 95.4899\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01126\n",
      "Average Test Loss: 0.3501; Test Acc: 88.4361\n",
      "-------------------------------------------\n",
      "Epoch:[88/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0498\n",
      "Average Train Loss: 0.1240; Train Acc: 95.6903\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00376\n",
      "Average Test Loss: 0.3381; Test Acc: 89.3172\n",
      "-------------------------------------------\n",
      "Epoch:[89/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.4848\n",
      "Average Train Loss: 0.1148; Train Acc: 96.1413\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00343\n",
      "Average Test Loss: 0.3307; Test Acc: 89.8678\n",
      "-------------------------------------------\n",
      "Epoch:[90/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.6268\n",
      "Average Train Loss: 0.1181; Train Acc: 95.9158\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00253\n",
      "Average Test Loss: 0.3268; Test Acc: 89.8678\n",
      "-------------------------------------------\n",
      "Epoch:[91/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0487\n",
      "Average Train Loss: 0.1137; Train Acc: 96.1413\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00199\n",
      "Average Test Loss: 0.3365; Test Acc: 90.3084\n",
      "-------------------------------------------\n",
      "Epoch:[92/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1098\n",
      "Average Train Loss: 0.1147; Train Acc: 95.9409\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00641\n",
      "Average Test Loss: 0.3237; Test Acc: 90.8590\n",
      "-------------------------------------------\n",
      "Epoch:[93/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0222\n",
      "Average Train Loss: 0.1117; Train Acc: 96.2415\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00518\n",
      "Average Test Loss: 0.3725; Test Acc: 88.2159\n",
      "-------------------------------------------\n",
      "Epoch:[94/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0377\n",
      "Average Train Loss: 0.1221; Train Acc: 95.6652\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00357\n",
      "Average Test Loss: 0.3316; Test Acc: 90.0881\n",
      "-------------------------------------------\n",
      "Epoch:[95/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.8515\n",
      "Average Train Loss: 0.1035; Train Acc: 96.4921\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00262\n",
      "Average Test Loss: 0.3603; Test Acc: 89.4273\n",
      "-------------------------------------------\n",
      "Epoch:[96/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0197\n",
      "Average Train Loss: 0.1045; Train Acc: 96.3418\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01180\n",
      "Average Test Loss: 0.3303; Test Acc: 90.3084\n",
      "-------------------------------------------\n",
      "Epoch:[97/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.0766\n",
      "Average Train Loss: 0.1069; Train Acc: 96.1163\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.01068\n",
      "Average Test Loss: 0.3448; Test Acc: 90.3084\n",
      "-------------------------------------------\n",
      "Epoch:[98/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.1313\n",
      "Average Train Loss: 0.1172; Train Acc: 96.0912\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00364\n",
      "Average Test Loss: 0.3501; Test Acc: 90.0881\n",
      "-------------------------------------------\n",
      "Epoch:[99/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.2893\n",
      "Average Train Loss: 0.1226; Train Acc: 95.8908\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00429\n",
      "Average Test Loss: 0.3548; Test Acc: 89.7577\n",
      "-------------------------------------------\n",
      "Epoch:[100/100]\n",
      "Training...\n",
      "Iter: [250/250] Loss: 0.3254\n",
      "Average Train Loss: 0.1178; Train Acc: 95.5400\n",
      "\n",
      "Testing...\n",
      "Iter: [908/908] Loss: 0.00576\n",
      "Average Test Loss: 0.3749; Test Acc: 88.4361\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "network = Classification(10, alignment=True).cuda()\n",
    "epochs = 100 # you can change the value to a small number for debugging\n",
    "\n",
    "## TASK 2.8\n",
    "# see Appendix C\n",
    "# choose an optimizer and an initial learning rate\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "# choose a lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,20,gamma=0.5)\n",
    "#######3\n",
    "\n",
    "# start training\n",
    "train_cls(train_cls_loader, test_cls_loader, network, optimizer, epochs, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the best test accuracy you can get.\n",
    "You can change the architecture, batch size, epochs and the scheduler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epoch:[56/60]\n",
    "Training...\n",
    "Iter: [250/250] Loss: 0.8768\n",
    "Average Train Loss: 0.1737; Train Acc: 94.0366\n",
    "\n",
    "Testing...\n",
    "Iter: [908/908] Loss: 0.01602\n",
    "Average Test Loss: 0.3337; Test Acc: 90.5286\n",
    "\n",
    "Epoch:[92/100]\n",
    "Training...\n",
    "Iter: [250/250] Loss: 0.1098\n",
    "Average Train Loss: 0.1147; Train Acc: 95.9409\n",
    "\n",
    "Testing...\n",
    "Iter: [908/908] Loss: 0.00641\n",
    "Average Test Loss: 0.3237; Test Acc: 90.8590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Segmentation Network\n",
    "In this network, you will use the global features and local features generated by the `Feature` network defined above.\n",
    "\n",
    "The global feature matrix is of size `B x 1024` and the local feature matrix is of size `B x 64 x N`.\n",
    "\n",
    "They need to be stacked together to a new matrix of size `B x 1088 x n` (How?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main train function for classification\n",
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, num_classes, alignment=False):\n",
    "        super(Segmentation, self).__init__()\n",
    "               \n",
    "        self.feature = Feature(alignment=alignment)\n",
    "\n",
    "        ## TASK 2.9\n",
    "        ## shared mlp\n",
    "        ## input size: B x 1088 x N\n",
    "        ## output size: B x num_classes x N\n",
    "        self.shared_mlp = nn.Sequential(nn.Conv1d(1088, 512, 1, stride=1),\n",
    "                              nn.BatchNorm1d(512),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv1d(512, 256, 1, stride=1),\n",
    "                              nn.BatchNorm1d(256),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Conv1d(256, 128, 1, stride=1), \n",
    "                              nn.BatchNorm1d(128), \n",
    "                              nn.ReLU(), \n",
    "                              nn.Conv1d(128, num_classes, 1, stride=1),\n",
    "                              nn.Softmax(dim=1))\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        g, l, trans = self.feature(x)\n",
    "        \n",
    "        ## TASK 2.10\n",
    "        # concat global features and local features to a single matrix\n",
    "        # g - B x 1024, global features\n",
    "        # l - B x 64 x N, local features\n",
    "        # x - B x 1088 x N, concatenated features\n",
    "        g = torch.repeat_interleave(g.view(-1,1024,1), repeats=l.shape[2], dim=2)\n",
    "        x = torch.cat((l, g),dim=1)\n",
    "            \n",
    "        ## TASK 2.9\n",
    "        ## forward of shared mlp\n",
    "        # input - B x 1088 x N\n",
    "        # output - B x num_classes x N  \n",
    "        x = self.shared_mlp(x)\n",
    "        x = self.logsoftmax(x)\n",
    "        \n",
    "        return x, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 10]), torch.Size([2, 64, 64]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random generate data and test this network\n",
    "segmentation = Segmentation(5,alignment=True)\n",
    "# B x num_classes x N \n",
    "x,trans = segmentation(torch.randn(2,3,10))\n",
    "x.shape,trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Calculating Intersection over Union (IoU) \n",
    "For 2D image, the IoU is calculated as follows,\n",
    "![iou](img/iou.png)\n",
    "\n",
    "How is it used in the literature of point clouds?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Evaluation metric is mIoU on points. For each shape S of category C, to calculate the shape’s mIoU: For each part type in category C, compute IoU between groundtruth and prediction. If the union of groundtruth and prediction points is empty, then count part IoU as 1. Then we average IoUs for all part types in category C to get mIoU for that shape. To calculate mIoU for the category, we take average of mIoUs for all shapes in that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK 2.11\n",
    "\n",
    "## If you don't my template, you are free to write your own function.\n",
    "\n",
    "# implement the helper functions to calculate the IoU\n",
    "def get_i_and_u(pred, target, num_classes):\n",
    "    \"\"\"Calculate intersection and union between pred and target.\n",
    "    \n",
    "    pred -- B x N matrix\n",
    "    target -- B x N matrix\n",
    "    num_classes -- number of classes\n",
    "    \n",
    "    return i, u\n",
    "    i -- B x C binary matrix, intersection, i[b, c] equals 1 if and only if it is a true-positive.\n",
    "    u -- B x C binary matrix, union, u[b, c] equals 0 if and only if it is a true-negative\n",
    "    \"\"\"\n",
    "    ## TASK 2.11\n",
    "    ## calculate i and u here\n",
    "    ## hint: useful function `F.one_hot`    \n",
    "    ## hint: use element-wise logical tensor operation (`&` and `|`)\n",
    "    target_onehot = F.one_hot(target, num_classes=num_classes)\n",
    "    pre_onehot = F.one_hot(pred, num_classes=num_classes)\n",
    "    i = torch.sum((target_onehot & pre_onehot).type(torch.float64), dim=1)\n",
    "    u = torch.sum((target_onehot | pre_onehot).type(torch.float64), dim=1)\n",
    "\n",
    "    return i, u\n",
    "\n",
    "def get_iou(pred, target, num_classes):\n",
    "    \"\"\"Calculate IoU\n",
    "    pred -- B x N matrix\n",
    "    target -- B x N matrix\n",
    "    num_classes -- number of classes\n",
    "    \n",
    "    return iou\n",
    "    iou -- B matrix, iou[b] is the IoU of b-th point cloud in this batch\n",
    "    \"\"\"\n",
    "    \n",
    "    ## use the helper function `i_and_u` defined above\n",
    "    i, u = get_i_and_u(pred, target, num_classes)\n",
    "    \n",
    "    ## TASK 2.11\n",
    "    ## calculate iou\n",
    "    iou = torch.sum(i,dim=1) / torch.sum(u,dim=1)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Train this network on ShapeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main train function for segmentation\n",
    "def train_seg(train_loader, test_loader, network, optimizer, epochs, scheduler):  \n",
    "    reg = OrthoLoss()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch:[{:02d}/{:02d}]'.format(epoch+1, epochs))\n",
    "        print('Training...')\n",
    "        network.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        ious = []\n",
    "        for batch, (pos, label) in enumerate(train_loader):\n",
    "            network.zero_grad()\n",
    "            pos, label = pos.cuda(), label.cuda()\n",
    "            ## TASK 2.12\n",
    "            ## forward propagation\n",
    "            output, trans = network(pos)\n",
    "            loss = nn.NLLLoss()(output,label) \n",
    "            ##########\n",
    "            if trans is not None:\n",
    "                loss += reg(trans) * 0.001        \n",
    "\n",
    "            pred = output.max(1)[1]\n",
    "            correct += pred.eq(label).sum().item()\n",
    "            total += label.numel()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            ious += [get_iou(pred, label, train_loader.dataset.num_classes)]\n",
    "            print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(train_loader), loss.item()), end='', flush=True)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print('\\nAverage Train Loss: {:.4f}; Train Acc: {:.4f}; Train mean IoU: {:.4f}'.format(train_loss/len(train_loader), correct/total * 100, torch.cat(ious, dim=0).mean().item()))\n",
    "\n",
    "        print('\\nTesting...')\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            ious = []\n",
    "            for batch, (pos, label) in enumerate(test_loader):\n",
    "                pos, label = pos.cuda(), label.cuda()\n",
    "                \n",
    "                ## TASK 2.12\n",
    "                ## forward propagation\n",
    "                output, trans = network(pos)\n",
    "                loss = nn.NLLLoss()(output,label) \n",
    "                ##########\n",
    "                \n",
    "                if trans is not None:\n",
    "                    loss += reg(trans) * 0.001   \n",
    "\n",
    "                pred = output.max(1)[1]\n",
    "                correct += pred.eq(label).sum().item()\n",
    "                total += label.numel()\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                ious += [get_iou(pred, label, train_loader.dataset.num_classes)]\n",
    "                print('\\rIter: [{:03d}/{:03d}] Loss: {:.4f}'.format(batch+1, len(test_loader), loss.item()), end='', flush=True)\n",
    "\n",
    "            print('\\nAverage Test Loss: {:.4f}; Test Acc: {:.4f}; Test mean IoU: {:.4f}'.format(test_loss/len(test_loader), correct/total * 100, torch.cat(ious, dim=0).mean().item()))\n",
    "        print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[01/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0927\n",
      "Average Train Loss: 1.1598; Train Acc: 79.1493; Train mean IoU: 0.6681\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0434\n",
      "Average Test Loss: 1.1083; Test Acc: 81.8806; Test mean IoU: 0.7072\n",
      "-------------------------------------------\n",
      "Epoch:[02/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0423\n",
      "Average Train Loss: 1.0614; Train Acc: 86.3619; Train mean IoU: 0.7661\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0254\n",
      "Average Test Loss: 1.0580; Test Acc: 85.8185; Test mean IoU: 0.7588\n",
      "-------------------------------------------\n",
      "Epoch:[03/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0300\n",
      "Average Train Loss: 1.0376; Train Acc: 88.0358; Train mean IoU: 0.7918\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0487\n",
      "Average Test Loss: 1.0727; Test Acc: 85.2770; Test mean IoU: 0.7555\n",
      "-------------------------------------------\n",
      "Epoch:[04/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0257\n",
      "Average Train Loss: 1.0316; Train Acc: 88.3621; Train mean IoU: 0.7973\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.1414\n",
      "Average Test Loss: 1.1055; Test Acc: 81.6290; Test mean IoU: 0.6982\n",
      "-------------------------------------------\n",
      "Epoch:[05/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9975\n",
      "Average Train Loss: 1.0240; Train Acc: 88.9591; Train mean IoU: 0.8066\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9957\n",
      "Average Test Loss: 1.0345; Test Acc: 87.5246; Test mean IoU: 0.7874\n",
      "-------------------------------------------\n",
      "Epoch:[06/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0415\n",
      "Average Train Loss: 1.0224; Train Acc: 88.9840; Train mean IoU: 0.8071\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9813\n",
      "Average Test Loss: 1.0297; Test Acc: 88.0332; Test mean IoU: 0.7945\n",
      "-------------------------------------------\n",
      "Epoch:[07/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0339\n",
      "Average Train Loss: 1.0144; Train Acc: 89.5959; Train mean IoU: 0.8165\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.2728\n",
      "Average Test Loss: 1.3206; Test Acc: 77.9161; Test mean IoU: 0.6489\n",
      "-------------------------------------------\n",
      "Epoch:[08/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0111\n",
      "Average Train Loss: 1.0186; Train Acc: 89.1696; Train mean IoU: 0.8101\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0121\n",
      "Average Test Loss: 1.0371; Test Acc: 86.9878; Test mean IoU: 0.7797\n",
      "-------------------------------------------\n",
      "Epoch:[09/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0397\n",
      "Average Train Loss: 1.0112; Train Acc: 89.7776; Train mean IoU: 0.8197\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.1269\n",
      "Average Test Loss: 1.1258; Test Acc: 79.0009; Test mean IoU: 0.6583\n",
      "-------------------------------------------\n",
      "Epoch:[10/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0421\n",
      "Average Train Loss: 1.0157; Train Acc: 89.3816; Train mean IoU: 0.8135\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0248\n",
      "Average Test Loss: 1.0659; Test Acc: 84.7530; Test mean IoU: 0.7481\n",
      "-------------------------------------------\n",
      "Epoch:[11/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0131\n",
      "Average Train Loss: 1.0104; Train Acc: 89.8122; Train mean IoU: 0.8204\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0177\n",
      "Average Test Loss: 1.0586; Test Acc: 85.0665; Test mean IoU: 0.7479\n",
      "-------------------------------------------\n",
      "Epoch:[12/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9910\n",
      "Average Train Loss: 1.0069; Train Acc: 90.0404; Train mean IoU: 0.8237\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9605\n",
      "Average Test Loss: 1.0639; Test Acc: 84.2954; Test mean IoU: 0.7390\n",
      "-------------------------------------------\n",
      "Epoch:[13/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0061\n",
      "Average Train Loss: 1.0033; Train Acc: 90.3336; Train mean IoU: 0.8283\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0073\n",
      "Average Test Loss: 1.0322; Test Acc: 87.8109; Test mean IoU: 0.7910\n",
      "-------------------------------------------\n",
      "Epoch:[14/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9781\n",
      "Average Train Loss: 0.9998; Train Acc: 90.6459; Train mean IoU: 0.8332\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9773\n",
      "Average Test Loss: 1.0257; Test Acc: 88.0695; Test mean IoU: 0.7944\n",
      "-------------------------------------------\n",
      "Epoch:[15/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0239\n",
      "Average Train Loss: 1.0038; Train Acc: 90.2645; Train mean IoU: 0.8274\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0232\n",
      "Average Test Loss: 1.0550; Test Acc: 84.9677; Test mean IoU: 0.7450\n",
      "-------------------------------------------\n",
      "Epoch:[16/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9969\n",
      "Average Train Loss: 1.0030; Train Acc: 90.3331; Train mean IoU: 0.8285\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9726\n",
      "Average Test Loss: 1.0556; Test Acc: 85.7299; Test mean IoU: 0.7592\n",
      "-------------------------------------------\n",
      "Epoch:[17/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0099\n",
      "Average Train Loss: 1.0035; Train Acc: 90.2455; Train mean IoU: 0.8272\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9585\n",
      "Average Test Loss: 1.0581; Test Acc: 85.4159; Test mean IoU: 0.7558\n",
      "-------------------------------------------\n",
      "Epoch:[18/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0083\n",
      "Average Train Loss: 1.0009; Train Acc: 90.4963; Train mean IoU: 0.8309\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.1336\n",
      "Average Test Loss: 1.0681; Test Acc: 84.4337; Test mean IoU: 0.7454\n",
      "-------------------------------------------\n",
      "Epoch:[19/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9803\n",
      "Average Train Loss: 0.9978; Train Acc: 90.7797; Train mean IoU: 0.8351\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9978\n",
      "Average Test Loss: 1.0481; Test Acc: 86.5242; Test mean IoU: 0.7734\n",
      "-------------------------------------------\n",
      "Epoch:[20/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0327\n",
      "Average Train Loss: 0.9963; Train Acc: 90.9090; Train mean IoU: 0.8374\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0443\n",
      "Average Test Loss: 1.0420; Test Acc: 87.3272; Test mean IoU: 0.7832\n",
      "-------------------------------------------\n",
      "Epoch:[21/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9880\n",
      "Average Train Loss: 0.9917; Train Acc: 91.3607; Train mean IoU: 0.8447\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9591\n",
      "Average Test Loss: 1.0312; Test Acc: 88.5303; Test mean IoU: 0.8033\n",
      "-------------------------------------------\n",
      "Epoch:[22/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9850\n",
      "Average Train Loss: 0.9889; Train Acc: 91.6161; Train mean IoU: 0.8488\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9766\n",
      "Average Test Loss: 1.0313; Test Acc: 88.5731; Test mean IoU: 0.8041\n",
      "-------------------------------------------\n",
      "Epoch:[23/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9937\n",
      "Average Train Loss: 0.9888; Train Acc: 91.6272; Train mean IoU: 0.8491\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9554\n",
      "Average Test Loss: 1.0481; Test Acc: 87.5951; Test mean IoU: 0.7880\n",
      "-------------------------------------------\n",
      "Epoch:[24/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9672\n",
      "Average Train Loss: 0.9876; Train Acc: 91.7342; Train mean IoU: 0.8507\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9595\n",
      "Average Test Loss: 1.0247; Test Acc: 88.5911; Test mean IoU: 0.8040\n",
      "-------------------------------------------\n",
      "Epoch:[25/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9719\n",
      "Average Train Loss: 0.9877; Train Acc: 91.7202; Train mean IoU: 0.8506\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9615\n",
      "Average Test Loss: 1.0301; Test Acc: 87.8955; Test mean IoU: 0.7937\n",
      "-------------------------------------------\n",
      "Epoch:[26/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9928\n",
      "Average Train Loss: 0.9868; Train Acc: 91.8104; Train mean IoU: 0.8520\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0542\n",
      "Average Test Loss: 1.0387; Test Acc: 87.2898; Test mean IoU: 0.7842\n",
      "-------------------------------------------\n",
      "Epoch:[27/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9680\n",
      "Average Train Loss: 0.9866; Train Acc: 91.8332; Train mean IoU: 0.8524\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9757\n",
      "Average Test Loss: 1.0761; Test Acc: 83.8080; Test mean IoU: 0.7287\n",
      "-------------------------------------------\n",
      "Epoch:[28/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9815\n",
      "Average Train Loss: 0.9858; Train Acc: 91.9155; Train mean IoU: 0.8538\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9785\n",
      "Average Test Loss: 1.0556; Test Acc: 87.0372; Test mean IoU: 0.7823\n",
      "-------------------------------------------\n",
      "Epoch:[29/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 1.0576\n",
      "Average Train Loss: 0.9847; Train Acc: 92.0334; Train mean IoU: 0.8558\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0337\n",
      "Average Test Loss: 1.0500; Test Acc: 86.1809; Test mean IoU: 0.7677\n",
      "-------------------------------------------\n",
      "Epoch:[30/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9882\n",
      "Average Train Loss: 0.9950; Train Acc: 91.0400; Train mean IoU: 0.8398\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: [341/341] Loss: 0.9953\n",
      "Average Test Loss: 1.0460; Test Acc: 85.7850; Test mean IoU: 0.7574\n",
      "-------------------------------------------\n",
      "Epoch:[31/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9749\n",
      "Average Train Loss: 0.9873; Train Acc: 91.7603; Train mean IoU: 0.8514\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9625\n",
      "Average Test Loss: 1.0163; Test Acc: 88.7891; Test mean IoU: 0.8071\n",
      "-------------------------------------------\n",
      "Epoch:[32/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9745\n",
      "Average Train Loss: 0.9853; Train Acc: 91.9575; Train mean IoU: 0.8547\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9615\n",
      "Average Test Loss: 1.0089; Test Acc: 89.5607; Test mean IoU: 0.8188\n",
      "-------------------------------------------\n",
      "Epoch:[33/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9775\n",
      "Average Train Loss: 0.9839; Train Acc: 92.1167; Train mean IoU: 0.8572\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.1202\n",
      "Average Test Loss: 1.1621; Test Acc: 74.0807; Test mean IoU: 0.5971\n",
      "-------------------------------------------\n",
      "Epoch:[34/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9782\n",
      "Average Train Loss: 0.9854; Train Acc: 91.9521; Train mean IoU: 0.8545\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9844\n",
      "Average Test Loss: 1.0497; Test Acc: 85.5197; Test mean IoU: 0.7535\n",
      "-------------------------------------------\n",
      "Epoch:[35/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9888\n",
      "Average Train Loss: 0.9827; Train Acc: 92.2300; Train mean IoU: 0.8591\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0028\n",
      "Average Test Loss: 1.0562; Test Acc: 84.7736; Test mean IoU: 0.7423\n",
      "-------------------------------------------\n",
      "Epoch:[36/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9771\n",
      "Average Train Loss: 0.9814; Train Acc: 92.3544; Train mean IoU: 0.8612\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0519\n",
      "Average Test Loss: 1.0181; Test Acc: 88.6016; Test mean IoU: 0.8026\n",
      "-------------------------------------------\n",
      "Epoch:[37/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9685\n",
      "Average Train Loss: 0.9785; Train Acc: 92.6375; Train mean IoU: 0.8659\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0017\n",
      "Average Test Loss: 1.0677; Test Acc: 83.6288; Test mean IoU: 0.7260\n",
      "-------------------------------------------\n",
      "Epoch:[38/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9843\n",
      "Average Train Loss: 0.9787; Train Acc: 92.6183; Train mean IoU: 0.8656\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9773\n",
      "Average Test Loss: 1.0406; Test Acc: 86.3598; Test mean IoU: 0.7684\n",
      "-------------------------------------------\n",
      "Epoch:[39/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9884\n",
      "Average Train Loss: 0.9774; Train Acc: 92.7406; Train mean IoU: 0.8675\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9889\n",
      "Average Test Loss: 1.0321; Test Acc: 87.1878; Test mean IoU: 0.7799\n",
      "-------------------------------------------\n",
      "Epoch:[40/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9837\n",
      "Average Train Loss: 0.9805; Train Acc: 92.4637; Train mean IoU: 0.8629\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9629\n",
      "Average Test Loss: 1.0614; Test Acc: 84.2909; Test mean IoU: 0.7359\n",
      "-------------------------------------------\n",
      "Epoch:[41/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9678\n",
      "Average Train Loss: 0.9762; Train Acc: 92.8734; Train mean IoU: 0.8699\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9486\n",
      "Average Test Loss: 1.0052; Test Acc: 89.8844; Test mean IoU: 0.8251\n",
      "-------------------------------------------\n",
      "Epoch:[42/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9893\n",
      "Average Train Loss: 0.9738; Train Acc: 93.1208; Train mean IoU: 0.8739\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9585\n",
      "Average Test Loss: 1.0135; Test Acc: 89.0415; Test mean IoU: 0.8116\n",
      "-------------------------------------------\n",
      "Epoch:[43/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9787\n",
      "Average Train Loss: 0.9738; Train Acc: 93.1123; Train mean IoU: 0.8738\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9498\n",
      "Average Test Loss: 0.9996; Test Acc: 90.4761; Test mean IoU: 0.8347\n",
      "-------------------------------------------\n",
      "Epoch:[44/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9967\n",
      "Average Train Loss: 0.9732; Train Acc: 93.1783; Train mean IoU: 0.8749\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0164\n",
      "Average Test Loss: 1.0046; Test Acc: 89.9825; Test mean IoU: 0.8272\n",
      "-------------------------------------------\n",
      "Epoch:[45/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9816\n",
      "Average Train Loss: 0.9722; Train Acc: 93.2716; Train mean IoU: 0.8765\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9805\n",
      "Average Test Loss: 1.0126; Test Acc: 89.1617; Test mean IoU: 0.8124\n",
      "-------------------------------------------\n",
      "Epoch:[46/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9752\n",
      "Average Train Loss: 0.9720; Train Acc: 93.3001; Train mean IoU: 0.8769\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9697\n",
      "Average Test Loss: 1.0094; Test Acc: 89.4762; Test mean IoU: 0.8175\n",
      "-------------------------------------------\n",
      "Epoch:[47/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9625\n",
      "Average Train Loss: 0.9713; Train Acc: 93.3435; Train mean IoU: 0.8776\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0384\n",
      "Average Test Loss: 1.0548; Test Acc: 84.9141; Test mean IoU: 0.7452\n",
      "-------------------------------------------\n",
      "Epoch:[48/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9982\n",
      "Average Train Loss: 0.9703; Train Acc: 93.4494; Train mean IoU: 0.8793\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9912\n",
      "Average Test Loss: 1.0257; Test Acc: 87.8167; Test mean IoU: 0.7902\n",
      "-------------------------------------------\n",
      "Epoch:[49/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9953\n",
      "Average Train Loss: 0.9685; Train Acc: 93.6397; Train mean IoU: 0.8827\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0747\n",
      "Average Test Loss: 1.0275; Test Acc: 87.6730; Test mean IoU: 0.7881\n",
      "-------------------------------------------\n",
      "Epoch:[50/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9687\n",
      "Average Train Loss: 0.9674; Train Acc: 93.7549; Train mean IoU: 0.8847\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9741\n",
      "Average Test Loss: 1.0032; Test Acc: 90.0918; Test mean IoU: 0.8287\n",
      "-------------------------------------------\n",
      "Epoch:[51/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9561\n",
      "Average Train Loss: 0.9673; Train Acc: 93.7532; Train mean IoU: 0.8846\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9753\n",
      "Average Test Loss: 1.0031; Test Acc: 90.0856; Test mean IoU: 0.8273\n",
      "-------------------------------------------\n",
      "Epoch:[52/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9582\n",
      "Average Train Loss: 0.9678; Train Acc: 93.7166; Train mean IoU: 0.8840\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9738\n",
      "Average Test Loss: 1.0005; Test Acc: 90.3703; Test mean IoU: 0.8334\n",
      "-------------------------------------------\n",
      "Epoch:[53/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9783\n",
      "Average Train Loss: 0.9660; Train Acc: 93.8902; Train mean IoU: 0.8869\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0462\n",
      "Average Test Loss: 1.0280; Test Acc: 87.6137; Test mean IoU: 0.7871\n",
      "-------------------------------------------\n",
      "Epoch:[54/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9605\n",
      "Average Train Loss: 0.9655; Train Acc: 93.9316; Train mean IoU: 0.8877\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0312\n",
      "Average Test Loss: 1.0091; Test Acc: 89.5065; Test mean IoU: 0.8185\n",
      "-------------------------------------------\n",
      "Epoch:[55/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9467\n",
      "Average Train Loss: 0.9650; Train Acc: 93.9805; Train mean IoU: 0.8885\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9992\n",
      "Average Test Loss: 1.0196; Test Acc: 88.4617; Test mean IoU: 0.8017\n",
      "-------------------------------------------\n",
      "Epoch:[56/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9521\n",
      "Average Train Loss: 0.9649; Train Acc: 93.9957; Train mean IoU: 0.8888\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0198\n",
      "Average Test Loss: 1.0328; Test Acc: 87.1558; Test mean IoU: 0.7790\n",
      "-------------------------------------------\n",
      "Epoch:[57/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9798\n",
      "Average Train Loss: 0.9647; Train Acc: 94.0257; Train mean IoU: 0.8893\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9658\n",
      "Average Test Loss: 1.0086; Test Acc: 89.5567; Test mean IoU: 0.8175\n",
      "-------------------------------------------\n",
      "Epoch:[58/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9639\n",
      "Average Train Loss: 0.9636; Train Acc: 94.1256; Train mean IoU: 0.8910\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0243\n",
      "Average Test Loss: 1.0252; Test Acc: 87.8812; Test mean IoU: 0.7913\n",
      "-------------------------------------------\n",
      "Epoch:[59/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9609\n",
      "Average Train Loss: 0.9630; Train Acc: 94.1809; Train mean IoU: 0.8920\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0181\n",
      "Average Test Loss: 1.0141; Test Acc: 89.0572; Test mean IoU: 0.8108\n",
      "-------------------------------------------\n",
      "Epoch:[60/100]\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: [147/147] Loss: 0.9634\n",
      "Average Train Loss: 0.9625; Train Acc: 94.2382; Train mean IoU: 0.8929\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9607\n",
      "Average Test Loss: 1.0016; Test Acc: 90.2477; Test mean IoU: 0.8309\n",
      "-------------------------------------------\n",
      "Epoch:[61/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9556\n",
      "Average Train Loss: 0.9606; Train Acc: 94.4288; Train mean IoU: 0.8963\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9521\n",
      "Average Test Loss: 1.0008; Test Acc: 90.3327; Test mean IoU: 0.8327\n",
      "-------------------------------------------\n",
      "Epoch:[62/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9538\n",
      "Average Train Loss: 0.9601; Train Acc: 94.4796; Train mean IoU: 0.8972\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9624\n",
      "Average Test Loss: 0.9963; Test Acc: 90.7927; Test mean IoU: 0.8408\n",
      "-------------------------------------------\n",
      "Epoch:[63/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9549\n",
      "Average Train Loss: 0.9595; Train Acc: 94.5480; Train mean IoU: 0.8983\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9646\n",
      "Average Test Loss: 0.9985; Test Acc: 90.5946; Test mean IoU: 0.8367\n",
      "-------------------------------------------\n",
      "Epoch:[64/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9679\n",
      "Average Train Loss: 0.9593; Train Acc: 94.5626; Train mean IoU: 0.8986\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0008\n",
      "Average Test Loss: 1.0116; Test Acc: 89.2554; Test mean IoU: 0.8138\n",
      "-------------------------------------------\n",
      "Epoch:[65/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9644\n",
      "Average Train Loss: 0.9590; Train Acc: 94.5934; Train mean IoU: 0.8991\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9755\n",
      "Average Test Loss: 0.9946; Test Acc: 90.9535; Test mean IoU: 0.8424\n",
      "-------------------------------------------\n",
      "Epoch:[66/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9613\n",
      "Average Train Loss: 0.9587; Train Acc: 94.6236; Train mean IoU: 0.8997\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9873\n",
      "Average Test Loss: 1.0098; Test Acc: 89.4222; Test mean IoU: 0.8167\n",
      "-------------------------------------------\n",
      "Epoch:[67/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9603\n",
      "Average Train Loss: 0.9586; Train Acc: 94.6404; Train mean IoU: 0.9000\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9518\n",
      "Average Test Loss: 0.9994; Test Acc: 90.4835; Test mean IoU: 0.8353\n",
      "-------------------------------------------\n",
      "Epoch:[68/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9695\n",
      "Average Train Loss: 0.9579; Train Acc: 94.7049; Train mean IoU: 0.9011\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9548\n",
      "Average Test Loss: 0.9942; Test Acc: 90.9951; Test mean IoU: 0.8430\n",
      "-------------------------------------------\n",
      "Epoch:[69/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9576\n",
      "Average Train Loss: 0.9583; Train Acc: 94.6695; Train mean IoU: 0.9005\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9473\n",
      "Average Test Loss: 0.9991; Test Acc: 90.5289; Test mean IoU: 0.8357\n",
      "-------------------------------------------\n",
      "Epoch:[70/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9517\n",
      "Average Train Loss: 0.9580; Train Acc: 94.6884; Train mean IoU: 0.9008\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0186\n",
      "Average Test Loss: 1.0014; Test Acc: 90.2826; Test mean IoU: 0.8310\n",
      "-------------------------------------------\n",
      "Epoch:[71/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9686\n",
      "Average Train Loss: 0.9580; Train Acc: 94.6896; Train mean IoU: 0.9008\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9679\n",
      "Average Test Loss: 1.0121; Test Acc: 89.1974; Test mean IoU: 0.8135\n",
      "-------------------------------------------\n",
      "Epoch:[72/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9493\n",
      "Average Train Loss: 0.9573; Train Acc: 94.7609; Train mean IoU: 0.9021\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9987\n",
      "Average Test Loss: 0.9966; Test Acc: 90.7597; Test mean IoU: 0.8394\n",
      "-------------------------------------------\n",
      "Epoch:[73/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9587\n",
      "Average Train Loss: 0.9575; Train Acc: 94.7406; Train mean IoU: 0.9017\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0074\n",
      "Average Test Loss: 0.9940; Test Acc: 91.0322; Test mean IoU: 0.8440\n",
      "-------------------------------------------\n",
      "Epoch:[74/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9479\n",
      "Average Train Loss: 0.9582; Train Acc: 94.6759; Train mean IoU: 0.9006\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9761\n",
      "Average Test Loss: 0.9913; Test Acc: 91.2979; Test mean IoU: 0.8476\n",
      "-------------------------------------------\n",
      "Epoch:[75/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9644\n",
      "Average Train Loss: 0.9571; Train Acc: 94.7789; Train mean IoU: 0.9024\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0038\n",
      "Average Test Loss: 1.0104; Test Acc: 89.3301; Test mean IoU: 0.8146\n",
      "-------------------------------------------\n",
      "Epoch:[76/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9593\n",
      "Average Train Loss: 0.9566; Train Acc: 94.8377; Train mean IoU: 0.9034\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0204\n",
      "Average Test Loss: 1.0017; Test Acc: 90.2412; Test mean IoU: 0.8303\n",
      "-------------------------------------------\n",
      "Epoch:[77/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9571\n",
      "Average Train Loss: 0.9569; Train Acc: 94.8028; Train mean IoU: 0.9028\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9848\n",
      "Average Test Loss: 1.0109; Test Acc: 89.2940; Test mean IoU: 0.8145\n",
      "-------------------------------------------\n",
      "Epoch:[78/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9533\n",
      "Average Train Loss: 0.9565; Train Acc: 94.8433; Train mean IoU: 0.9035\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9947\n",
      "Average Test Loss: 0.9975; Test Acc: 90.7016; Test mean IoU: 0.8390\n",
      "-------------------------------------------\n",
      "Epoch:[79/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9452\n",
      "Average Train Loss: 0.9568; Train Acc: 94.8116; Train mean IoU: 0.9030\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9748\n",
      "Average Test Loss: 0.9986; Test Acc: 90.5674; Test mean IoU: 0.8359\n",
      "-------------------------------------------\n",
      "Epoch:[80/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9533\n",
      "Average Train Loss: 0.9560; Train Acc: 94.8991; Train mean IoU: 0.9045\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9826\n",
      "Average Test Loss: 0.9945; Test Acc: 90.9692; Test mean IoU: 0.8429\n",
      "-------------------------------------------\n",
      "Epoch:[81/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9489\n",
      "Average Train Loss: 0.9554; Train Acc: 94.9582; Train mean IoU: 0.9056\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9919\n",
      "Average Test Loss: 0.9942; Test Acc: 91.0225; Test mean IoU: 0.8440\n",
      "-------------------------------------------\n",
      "Epoch:[82/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9425\n",
      "Average Train Loss: 0.9546; Train Acc: 95.0366; Train mean IoU: 0.9069\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0021\n",
      "Average Test Loss: 0.9944; Test Acc: 90.9745; Test mean IoU: 0.8436\n",
      "-------------------------------------------\n",
      "Epoch:[83/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9489\n",
      "Average Train Loss: 0.9545; Train Acc: 95.0403; Train mean IoU: 0.9070\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0004\n",
      "Average Test Loss: 0.9946; Test Acc: 90.9722; Test mean IoU: 0.8429\n",
      "-------------------------------------------\n",
      "Epoch:[84/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9578\n",
      "Average Train Loss: 0.9545; Train Acc: 95.0485; Train mean IoU: 0.9071\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9986\n",
      "Average Test Loss: 0.9926; Test Acc: 91.1691; Test mean IoU: 0.8458\n",
      "-------------------------------------------\n",
      "Epoch:[85/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9491\n",
      "Average Train Loss: 0.9546; Train Acc: 95.0363; Train mean IoU: 0.9069\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0114\n",
      "Average Test Loss: 0.9956; Test Acc: 90.8551; Test mean IoU: 0.8406\n",
      "-------------------------------------------\n",
      "Epoch:[86/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9501\n",
      "Average Train Loss: 0.9540; Train Acc: 95.0979; Train mean IoU: 0.9080\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9780\n",
      "Average Test Loss: 0.9926; Test Acc: 91.1585; Test mean IoU: 0.8460\n",
      "-------------------------------------------\n",
      "Epoch:[87/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9558\n",
      "Average Train Loss: 0.9541; Train Acc: 95.0963; Train mean IoU: 0.9080\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0240\n",
      "Average Test Loss: 1.0078; Test Acc: 89.6439; Test mean IoU: 0.8203\n",
      "-------------------------------------------\n",
      "Epoch:[88/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9568\n",
      "Average Train Loss: 0.9540; Train Acc: 95.1015; Train mean IoU: 0.9081\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0059\n",
      "Average Test Loss: 0.9924; Test Acc: 91.2033; Test mean IoU: 0.8467\n",
      "-------------------------------------------\n",
      "Epoch:[89/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9686\n",
      "Average Train Loss: 0.9540; Train Acc: 95.0983; Train mean IoU: 0.9080\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: [341/341] Loss: 0.9885\n",
      "Average Test Loss: 0.9927; Test Acc: 91.1554; Test mean IoU: 0.8461\n",
      "-------------------------------------------\n",
      "Epoch:[90/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9460\n",
      "Average Train Loss: 0.9538; Train Acc: 95.1115; Train mean IoU: 0.9082\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0051\n",
      "Average Test Loss: 0.9925; Test Acc: 91.1823; Test mean IoU: 0.8462\n",
      "-------------------------------------------\n",
      "Epoch:[91/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9575\n",
      "Average Train Loss: 0.9538; Train Acc: 95.1170; Train mean IoU: 0.9084\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9964\n",
      "Average Test Loss: 0.9959; Test Acc: 90.8286; Test mean IoU: 0.8403\n",
      "-------------------------------------------\n",
      "Epoch:[92/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9689\n",
      "Average Train Loss: 0.9537; Train Acc: 95.1258; Train mean IoU: 0.9085\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9930\n",
      "Average Test Loss: 0.9923; Test Acc: 91.2012; Test mean IoU: 0.8464\n",
      "-------------------------------------------\n",
      "Epoch:[93/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9566\n",
      "Average Train Loss: 0.9534; Train Acc: 95.1640; Train mean IoU: 0.9092\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9689\n",
      "Average Test Loss: 0.9995; Test Acc: 90.4648; Test mean IoU: 0.8340\n",
      "-------------------------------------------\n",
      "Epoch:[94/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9593\n",
      "Average Train Loss: 0.9537; Train Acc: 95.1267; Train mean IoU: 0.9085\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9800\n",
      "Average Test Loss: 0.9915; Test Acc: 91.2881; Test mean IoU: 0.8477\n",
      "-------------------------------------------\n",
      "Epoch:[95/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9591\n",
      "Average Train Loss: 0.9536; Train Acc: 95.1382; Train mean IoU: 0.9087\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0292\n",
      "Average Test Loss: 1.0033; Test Acc: 90.0966; Test mean IoU: 0.8283\n",
      "-------------------------------------------\n",
      "Epoch:[96/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9585\n",
      "Average Train Loss: 0.9532; Train Acc: 95.1748; Train mean IoU: 0.9094\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9843\n",
      "Average Test Loss: 0.9926; Test Acc: 91.1883; Test mean IoU: 0.8470\n",
      "-------------------------------------------\n",
      "Epoch:[97/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9633\n",
      "Average Train Loss: 0.9530; Train Acc: 95.2041; Train mean IoU: 0.9099\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9954\n",
      "Average Test Loss: 0.9912; Test Acc: 91.3162; Test mean IoU: 0.8489\n",
      "-------------------------------------------\n",
      "Epoch:[98/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9562\n",
      "Average Train Loss: 0.9530; Train Acc: 95.1956; Train mean IoU: 0.9097\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 1.0054\n",
      "Average Test Loss: 0.9917; Test Acc: 91.2605; Test mean IoU: 0.8477\n",
      "-------------------------------------------\n",
      "Epoch:[99/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9567\n",
      "Average Train Loss: 0.9528; Train Acc: 95.2143; Train mean IoU: 0.9101\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9841\n",
      "Average Test Loss: 0.9994; Test Acc: 90.4676; Test mean IoU: 0.8348\n",
      "-------------------------------------------\n",
      "Epoch:[100/100]\n",
      "Training...\n",
      "Iter: [147/147] Loss: 0.9446\n",
      "Average Train Loss: 0.9528; Train Acc: 95.2127; Train mean IoU: 0.9100\n",
      "\n",
      "Testing...\n",
      "Iter: [341/341] Loss: 0.9747\n",
      "Average Test Loss: 1.0000; Test Acc: 90.4184; Test mean IoU: 0.8332\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "network = Segmentation(train_seg_dataset.num_classes, alignment=True).cuda()\n",
    "epochs = 100 # you can change the value to a small number for debugging\n",
    "\n",
    "## TASK 2.13\n",
    "# see Appendix C\n",
    "# choose an optimizer and an initial learning rate\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=0.001)\n",
    "# choose a lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,20,gamma=0.5)\n",
    "#######3\n",
    "\n",
    "train_seg(train_seg_loader, test_seg_loader, network, optimizer, epochs, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the best test mIoU you can get."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Epoch:[97/100]\n",
    "Training...\n",
    "Iter: [147/147] Loss: 0.9633\n",
    "Average Train Loss: 0.9530; Train Acc: 95.2041; Train mean IoU: 0.9099\n",
    "\n",
    "Testing...\n",
    "Iter: [341/341] Loss: 0.9954\n",
    "Average Test Loss: 0.9912; Test Acc: 91.3162; Test mean IoU: 0.8489"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 PointNet++ and DGCNN (https://arxiv.org/abs/1801.07829)\n",
    "Read these two papers and answer:\n",
    "1. What's the major difference over PointNet?\n",
    "2. If you are going to implement PointNet++ or DGCNN, describe your plan briefly (what kind of modules/layers or functions you need based on this project)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## TASK 2.14\n",
    "## No coding!! Put your answer here.\n",
    "1. \n",
    "    1. The basic idea of PointNet is to learn a spatial encoding of each point and then aggregate all individual point features to a global point cloud signature. It does not capture local structure induced by the metric\n",
    "    2. Since exploiting local structure has proven to be important for the success of convolutional architectures, Point++ introduces a hierarchical neural network to process a set of points sampled in a metric space in a hierarchical fashion, which is able to learn local features even in non-uniformly sampled point sets. \n",
    "    3. PointNet as a local feature learner is applied recursively on a nested partitioning of the input set by PointNet++ for abstracting the local features.\n",
    "    \n",
    "2. \n",
    "    1.Pointnet++: The input go through sampling layer, grouping layer and then pointnet layer. Those three layer called set abstraction. Repeat set abstraction for serval times. Then, for classificaion problems, it goes though a pointnet and then fully connected layers. For segmentation, it goes through repeat of interpolate and unit pointnet to predict pre-point score.\n",
    "    2. DCGAN: The architecture is similar to Pointnet, which also has a spatial transformer which computes a global shape transformation. There are two architecture for two tasks: classification and segmentation.   Classification: Consists of 2 EdgeConv layer (The first layer uses 3 fully connected layers (64,64,64) while the second layer uses a shared fully connected layer(128)) followed by pooling operation and 3 fully-connected layers to transform the global feature, producing the k-class score. All layer includes ReLU and batch normalization.\n",
    " Segmentation: Consists of a sequence of 3 EdgeConv layers followed by 3 fully-connected layers producing a k-class score for each point.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
