{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LONG Yongkang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed resources\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import requests\n",
    "from torchvision import transforms, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-SXM2-32GB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size = 400, shape = None):\n",
    "    \n",
    "    ''' Load and downscale an image if the shorter side is longer than <max_size> px '''\n",
    "    \n",
    "    if \"http\" in img_path:\n",
    "        response = requests.get(img_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    if min(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = min(image.size)\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "    \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard alpha channel (:3) and append the batch dimension (unsqueeze(0))\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for un-normalizing an image \n",
    "# and converting it from a Tensor image to a NumPy image for display\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Convert a PyTorch tensor to a NumPy image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model, layers):\n",
    "    # Run an image forward through a model and get the features for a set of layers.\n",
    "        \n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor, normalize = False):\n",
    "    \"\"\" Calculate the Gram Matrix of a given tensor \n",
    "        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO Task 1: Implement the computation of the Gram Matrix\n",
    "    \n",
    "    ## get the batch_size b, depth c, height h, and width w of the Tensor\n",
    "    ## reshape it, so we're multiplying the features for each channel\n",
    "    ## calculate the gram matrix\n",
    "    b,c, h, w = tensor.size()\n",
    "    tensor = tensor.view(b*c, h * w)\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    ## if normalize = True, normalize the gram matrix as it is done in Equation 3 of [5]\n",
    "    if normalize == True:\n",
    "         gram.div(b * c * h * w)\n",
    "    #gram = None\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptual Losses for Real-Time Style Transfer and Super-Resolution [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of solving an optimization problem for each target image, Johnson et al. [5] train a feed forward neural network for style transfer. Your task now is to re-implement certain aspects of their method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions and classes, you do not have to change anything here, you can however if these functions do not fully satisfy your needs\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.total_imgs = os.listdir(root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return load_image(os.path.join(self.root, self.total_imgs[idx]))[0]\n",
    "    \n",
    "def get_checkpoint_dir(suffix = None):\n",
    "    max_num = 0\n",
    "\n",
    "    dirs = [os.path.basename(x[0]) for x in os.walk('checkpoints')]\n",
    "\n",
    "    for dir in dirs:\n",
    "        elems = dir.split('_')\n",
    "        if len(elems) > 0:\n",
    "            prefix = elems[0]\n",
    "\n",
    "            if len(prefix) > 0 and all(map(str.isdigit, prefix)):\n",
    "                num = int(prefix)\n",
    "\n",
    "                if max_num <= num:\n",
    "                    max_num = num + 1\n",
    "\n",
    "    checkpoint_dir = os.path.join('checkpoints', str(max_num).zfill(5) + '_checkpoint_' + (suffix if not suffix == None else ''))\n",
    "\n",
    "    return max_num, checkpoint_dir\n",
    "\n",
    "def write_checkpoint(checkpoint_dir, model, optimizer, iter_number):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    torch.save({\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, os.path.join(checkpoint_dir, str(iter_number).zfill(6) + '.pth'))\n",
    "\n",
    "def load_checkpoint(pth_path, model, optimizer = None):\n",
    "    checkpoint = torch.load(pth_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    if not optimizer is None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1: Implement the Residual Block (5 points)\n",
    "\n",
    "The authors of [5] explain the details about the proposed neural network in [6]. Start with implementing the residual block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # TODO Task 5.1\n",
    "        self.conv1 = ConvolutionalBlock(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvolutionalBlock(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO Task 5.1\n",
    "        residual = x\n",
    "        out = self.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        out = out + residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.2: Implement the Convolutional Block (5 points)\n",
    "\n",
    "Refer to [5] and [6] for details. Note that the convolutional block shown in [6] in Figure 1 is only for explanatory reasons. Here the goal is to implement a convolutional block consisting of one convolutional layer, batch normalization and activation layer. However, feel free to adapt the structure, e.g. also to implement a convolutional block consisting of two conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=1):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        \n",
    "        # TODO Task 5.2\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO Task 5.2\n",
    "        x = self.reflection_pad(x)\n",
    "        x = self.conv2d(x)\n",
    "        #x=F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.3: Implement the Deconvolutional Block (5 points)\n",
    "\n",
    "In [6], the authors refer to the deconvolution as a convolution with a stride of 1/2. In PyTorch, this is achieved using a `nn.ConvTranspose2d`  layer. Be careful when setting values for `padding` and `output_padding`, such that the output size matches the size explained in [6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(DeconvolutionalBlock, self).__init__()\n",
    "        \n",
    "        # TODO Task 5.3\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.upsample_layer = torch.nn.Upsample(scale_factor=upsample)\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO Task 5.3\n",
    "        if self.upsample:\n",
    "            x = self.upsample_layer(x)\n",
    "        x = self.reflection_pad(x)\n",
    "        x = self.conv2d(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Put it all together (5 points)\n",
    "\n",
    "Using the blocks from Tasks 5.1 to 5.3, assemble a neural network structure for style transfer (so not the one for super resolution) as explained in [5] and [6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleTransferNetwork, self).__init__()\n",
    "        \n",
    "        # TODO Task 5.4\n",
    "        # Initial convolution layers\n",
    "        self.dim = 128\n",
    "        self.conv1 = ConvolutionalBlock(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvolutionalBlock(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvolutionalBlock(64, self.dim, kernel_size=3, stride=2)\n",
    "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(self.dim)\n",
    "        self.res2 = ResidualBlock(self.dim)\n",
    "        self.res3 = ResidualBlock(self.dim)\n",
    "        self.res4 = ResidualBlock(self.dim)\n",
    "        self.res5 = ResidualBlock(self.dim)\n",
    "        # Deconvolutional Layers\n",
    "        self.deconv1 = DeconvolutionalBlock(self.dim, 64, kernel_size=3, stride=1,upsample=None)\n",
    "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.deconv2 = DeconvolutionalBlock(64, 32, kernel_size=3, stride=1, upsample=None)\n",
    "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.deconv3 = ConvolutionalBlock(32, 3, kernel_size=9, stride=1)\n",
    "        # Non-linearities\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO Task 5.4\n",
    "        x = self.relu(self.in1(self.conv1(x)))\n",
    "        x = self.relu(self.in2(self.conv2(x)))\n",
    "        x = self.relu(self.in3(self.conv3(x)))\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.res5(x)\n",
    "        x = self.relu(self.in4(self.deconv1(x)))\n",
    "        x = self.relu(self.in5(self.deconv2(x)))\n",
    "        x = self.deconv3(x)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1.0e-3\n",
    "batch_size = 4\n",
    "num_epochs = 2\n",
    "\n",
    "content_weight = 5e0\n",
    "style_weight = 1e2\n",
    "tv_weight = 1e-6\n",
    "\n",
    "plot_every = 5000 # 1000\n",
    "checkpoint_every =  5000\n",
    "\n",
    "content_img_name = 'chicago'\n",
    "content_img_path = os.path.join('img', 'content', content_img_name + '.jpg')\n",
    "content_torch = load_image(content_img_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Style: mosaic\n"
     ]
    }
   ],
   "source": [
    "# TODO: Choose a style that you like and that you would like to train your model on by setting style_idx appropriately\n",
    "\n",
    "style_names = ['candy', 'composition_vii', 'feathers', 'la_muse', 'mosaic', 'starry_night_crop', 'the_scream' 'udnie', 'wave_crop']\n",
    "style_idx = 4\n",
    "\n",
    "print('Chosen Style:', style_names[style_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similiar to the authors of [5], we want to use vgg16 this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG 16\n",
    "# get the \"features\" portion of VGG16 (we will not need the \"classifier\" portion)\n",
    "vgg16 = models.vgg16(pretrained = True).features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.1 Complete the training method, refer to [5] and [6] for details (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(style_name):\n",
    "\n",
    "    coco_dir = 'MSCOCO_256x256'\n",
    "\n",
    "    train_dataset = ImageDataset(root = os.path.join(coco_dir, 'train2014'))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = 4,\n",
    "        num_workers = 0,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    model = StyleTransferNetwork().to(device)\n",
    "\n",
    "    learning_curve = []\n",
    "\n",
    "    # freeze all VGG parameters since we're only optimizing the target image\n",
    "    for param in vgg16.parameters():\n",
    "        param.requires_grad_(False)\n",
    "        \n",
    "    # TODO Task 6.1: Similar as it was done before, find the correspondence of all image numbers and image names in vgg16 that are used in [5] for style transfer. You can again use the print(vgg16) command.\n",
    "    layers_vgg = {'3':'relu1_2',\n",
    "                  '8':'relu2_2',\n",
    "                  '15':'relu3_3',\n",
    "                  '22':'relu4_3'}\n",
    "\n",
    "    num_args = len(sys.argv)\n",
    "\n",
    "    style_image_path = os.path.join('img', 'style', style_name + '.jpg')\n",
    "    style_image = load_image(style_image_path).to(device)\n",
    "\n",
    "    style_features = get_features(style_image, vgg16, layers_vgg)\n",
    "    style_grams = {layer: gram_matrix(style_features[layer], normalize = True) for layer in style_features}\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    # TODO Task 6.1: Find the name of the content layer in vgg16 that is used in [5] for style transfer\n",
    "    content_layer = 'relu2_2'\n",
    "\n",
    "    # TODO Task 6.1: Find the names of the style layers in vgg16 that is used in [5] for style transfer and create a dictionary with entries 'name': 1.0\n",
    "    style_weights = {'relu1_2': 1.0,\n",
    "                     'relu2_2':1.0,\n",
    "                     'relu3_3':1.0,\n",
    "                     'relu4_3':1.0}\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    num_iters = num_epochs * num_batches\n",
    "\n",
    "    num_iter = 0\n",
    "\n",
    "    weight_string = '_content_weight_{}_style_weight_{}_tv_weight_{}'.format(content_weight, style_weight, tv_weight)\n",
    "    run_id, checkpoint_dir = get_checkpoint_dir(style_names[style_idx] + weight_string)\n",
    "    \n",
    "    print(\"Writing checkpoints to\", checkpoint_dir)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for batch_idx, images in enumerate(train_loader):\n",
    "\n",
    "            b, c, h, w = images.shape\n",
    "\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # TODO Task 6.1: Get the features for the current batch of images\n",
    "            #content_features = None    ###Do not used [] outside for style_features\n",
    "            content_features = get_features(images, vgg16, layers_vgg)\n",
    "            \n",
    "            # TODO Task 6.1: Send the batch of images through the network\n",
    "            out = None\n",
    "            out = vgg16(images)\n",
    "\n",
    "            # TODO Task 6.1: Get the features for the model output\n",
    "            target_features = None\n",
    "            target_features = get_features(images.data.clone().requires_grad_(True).to(device), vgg16,layers_vgg)\n",
    "        \n",
    "            target_grams = {layer: gram_matrix(target_features[layer], normalize=True) for layer in target_features}\n",
    "\n",
    "            # Content Loss (Feature Loss)\n",
    "            content_loss = torch.mean((target_features[content_layer] - content_features[content_layer]) ** 2)\n",
    "\n",
    "            # Style Loss\n",
    "            style_loss = 0\n",
    "\n",
    "            # then add to it for each layer's gram matrix loss\n",
    "            for layer in style_weights:\n",
    "                target_gram = target_grams[layer]\n",
    "                style_gram = style_grams[layer]\n",
    "                layer_style_loss= style_weights[layer] *\\\n",
    "                torch.mean((target_gram - style_gram.repeat(b,b)) ** 2)/b\n",
    "                style_loss += layer_style_loss / (h * w)**2\n",
    "                \n",
    "            ## TODO Task 6.1: Compute the anisotropic Total Variation loss of out according to https://en.wikipedia.org/wiki/Total_variation_denoising\n",
    "            tv_loss = 0\n",
    "            tv_loss = torch.sum(torch.abs(out[:, :, :, :-1] - out[:, :, :, 1:]))+\\\n",
    "            torch.sum(torch.abs(out[:, :, :-1, :] - out[:, :, 1:, :]))\n",
    "\n",
    "\n",
    "            # TODO Task 6.1: Compute the total weighted loss consisting of content loss, style loss and total variation loss\n",
    "            total_loss =  content_weight * content_loss + style_weight * style_loss + tv_weight * tv_loss\n",
    "\n",
    "            running_loss += total_loss\n",
    "\n",
    "            # update weights\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_iter += 1\n",
    "\n",
    "            if num_iter % plot_every == 0:\n",
    "                model.eval()\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, num_iter / num_iters),\n",
    "                                                    num_iter, num_iter / num_iters * 100, running_loss / plot_every))\n",
    "                \n",
    "                learning_curve.append(running_loss / plot_every)\n",
    "\n",
    "                # Send current state image to Tensorboard\n",
    "                out = model(content_torch)\n",
    "\n",
    "                plt.imshow(im_convert(out))\n",
    "                plt.show()\n",
    "\n",
    "                running_loss = 0\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            if num_iter % checkpoint_every == 0:\n",
    "                # Write Checkpoint\n",
    "                write_checkpoint(checkpoint_dir, model, optimizer, num_iter)\n",
    "                \n",
    "    return model, np.asarray(learning_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing checkpoints to checkpoints/00010_checkpoint_mosaic_content_weight_5.0_style_weight_100.0_tv_weight_1e-06\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.92 GiB total capacity; 9.09 GiB already allocated; 59.44 MiB free; 1.22 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2f2408bd6211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_curvce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstyle_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-e8cb0f644683>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(style_name)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.92 GiB total capacity; 9.09 GiB already allocated; 59.44 MiB free; 1.22 GiB cached)"
     ]
    }
   ],
   "source": [
    "model, learning_curvce = train_model(style_names[style_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for StyleTransferNetwork:\n\tMissing key(s) in state_dict: \"conv1.conv2d.weight\", \"conv1.conv2d.bias\", \"in1.weight\", \"in1.bias\", \"conv2.conv2d.weight\", \"conv2.conv2d.bias\", \"in2.weight\", \"in2.bias\", \"conv3.conv2d.weight\", \"conv3.conv2d.bias\", \"in3.weight\", \"in3.bias\", \"res1.conv1.conv2d.weight\", \"res1.conv1.conv2d.bias\", \"res1.in1.weight\", \"res1.in1.bias\", \"res1.conv2.conv2d.weight\", \"res1.conv2.conv2d.bias\", \"res1.in2.weight\", \"res1.in2.bias\", \"res2.conv1.conv2d.weight\", \"res2.conv1.conv2d.bias\", \"res2.in1.weight\", \"res2.in1.bias\", \"res2.conv2.conv2d.weight\", \"res2.conv2.conv2d.bias\", \"res2.in2.weight\", \"res2.in2.bias\", \"res3.conv1.conv2d.weight\", \"res3.conv1.conv2d.bias\", \"res3.in1.weight\", \"res3.in1.bias\", \"res3.conv2.conv2d.weight\", \"res3.conv2.conv2d.bias\", \"res3.in2.weight\", \"res3.in2.bias\", \"res4.conv1.conv2d.weight\", \"res4.conv1.conv2d.bias\", \"res4.in1.weight\", \"res4.in1.bias\", \"res4.conv2.conv2d.weight\", \"res4.conv2.conv2d.bias\", \"res4.in2.weight\", \"res4.in2.bias\", \"res5.conv1.conv2d.weight\", \"res5.conv1.conv2d.bias\", \"res5.in1.weight\", \"res5.in1.bias\", \"res5.conv2.conv2d.weight\", \"res5.conv2.conv2d.bias\", \"res5.in2.weight\", \"res5.in2.bias\", \"deconv1.conv2d.weight\", \"deconv1.conv2d.bias\", \"in4.weight\", \"in4.bias\", \"deconv2.conv2d.weight\", \"deconv2.conv2d.bias\", \"in5.weight\", \"in5.bias\", \"deconv3.conv2d.weight\", \"deconv3.conv2d.bias\". \n\tUnexpected key(s) in state_dict: \"ConvBlock.0.conv2d.weight\", \"ConvBlock.0.conv2d.bias\", \"ConvBlock.0.batchNorm.weight\", \"ConvBlock.0.batchNorm.bias\", \"ConvBlock.0.batchNorm.running_mean\", \"ConvBlock.0.batchNorm.running_var\", \"ConvBlock.0.batchNorm.num_batches_tracked\", \"ConvBlock.2.conv2d.weight\", \"ConvBlock.2.conv2d.bias\", \"ConvBlock.2.batchNorm.weight\", \"ConvBlock.2.batchNorm.bias\", \"ConvBlock.2.batchNorm.running_mean\", \"ConvBlock.2.batchNorm.running_var\", \"ConvBlock.2.batchNorm.num_batches_tracked\", \"ConvBlock.4.conv2d.weight\", \"ConvBlock.4.conv2d.bias\", \"ConvBlock.4.batchNorm.weight\", \"ConvBlock.4.batchNorm.bias\", \"ConvBlock.4.batchNorm.running_mean\", \"ConvBlock.4.batchNorm.running_var\", \"ConvBlock.4.batchNorm.num_batches_tracked\", \"ResBlock.0.conv1.conv2d.weight\", \"ResBlock.0.conv1.conv2d.bias\", \"ResBlock.0.conv1.batchNorm.weight\", \"ResBlock.0.conv1.batchNorm.bias\", \"ResBlock.0.conv1.batchNorm.running_mean\", \"ResBlock.0.conv1.batchNorm.running_var\", \"ResBlock.0.conv1.batchNorm.num_batches_tracked\", \"ResBlock.0.conv2.conv2d.weight\", \"ResBlock.0.conv2.conv2d.bias\", \"ResBlock.0.conv2.batchNorm.weight\", \"ResBlock.0.conv2.batchNorm.bias\", \"ResBlock.0.conv2.batchNorm.running_mean\", \"ResBlock.0.conv2.batchNorm.running_var\", \"ResBlock.0.conv2.batchNorm.num_batches_tracked\", \"ResBlock.1.conv1.conv2d.weight\", \"ResBlock.1.conv1.conv2d.bias\", \"ResBlock.1.conv1.batchNorm.weight\", \"ResBlock.1.conv1.batchNorm.bias\", \"ResBlock.1.conv1.batchNorm.running_mean\", \"ResBlock.1.conv1.batchNorm.running_var\", \"ResBlock.1.conv1.batchNorm.num_batches_tracked\", \"ResBlock.1.conv2.conv2d.weight\", \"ResBlock.1.conv2.conv2d.bias\", \"ResBlock.1.conv2.batchNorm.weight\", \"ResBlock.1.conv2.batchNorm.bias\", \"ResBlock.1.conv2.batchNorm.running_mean\", \"ResBlock.1.conv2.batchNorm.running_var\", \"ResBlock.1.conv2.batchNorm.num_batches_tracked\", \"ResBlock.2.conv1.conv2d.weight\", \"ResBlock.2.conv1.conv2d.bias\", \"ResBlock.2.conv1.batchNorm.weight\", \"ResBlock.2.conv1.batchNorm.bias\", \"ResBlock.2.conv1.batchNorm.running_mean\", \"ResBlock.2.conv1.batchNorm.running_var\", \"ResBlock.2.conv1.batchNorm.num_batches_tracked\", \"ResBlock.2.conv2.conv2d.weight\", \"ResBlock.2.conv2.conv2d.bias\", \"ResBlock.2.conv2.batchNorm.weight\", \"ResBlock.2.conv2.batchNorm.bias\", \"ResBlock.2.conv2.batchNorm.running_mean\", \"ResBlock.2.conv2.batchNorm.running_var\", \"ResBlock.2.conv2.batchNorm.num_batches_tracked\", \"ResBlock.3.conv1.conv2d.weight\", \"ResBlock.3.conv1.conv2d.bias\", \"ResBlock.3.conv1.batchNorm.weight\", \"ResBlock.3.conv1.batchNorm.bias\", \"ResBlock.3.conv1.batchNorm.running_mean\", \"ResBlock.3.conv1.batchNorm.running_var\", \"ResBlock.3.conv1.batchNorm.num_batches_tracked\", \"ResBlock.3.conv2.conv2d.weight\", \"ResBlock.3.conv2.conv2d.bias\", \"ResBlock.3.conv2.batchNorm.weight\", \"ResBlock.3.conv2.batchNorm.bias\", \"ResBlock.3.conv2.batchNorm.running_mean\", \"ResBlock.3.conv2.batchNorm.running_var\", \"ResBlock.3.conv2.batchNorm.num_batches_tracked\", \"ResBlock.4.conv1.conv2d.weight\", \"ResBlock.4.conv1.conv2d.bias\", \"ResBlock.4.conv1.batchNorm.weight\", \"ResBlock.4.conv1.batchNorm.bias\", \"ResBlock.4.conv1.batchNorm.running_mean\", \"ResBlock.4.conv1.batchNorm.running_var\", \"ResBlock.4.conv1.batchNorm.num_batches_tracked\", \"ResBlock.4.conv2.conv2d.weight\", \"ResBlock.4.conv2.conv2d.bias\", \"ResBlock.4.conv2.batchNorm.weight\", \"ResBlock.4.conv2.batchNorm.bias\", \"ResBlock.4.conv2.batchNorm.running_mean\", \"ResBlock.4.conv2.batchNorm.running_var\", \"ResBlock.4.conv2.batchNorm.num_batches_tracked\", \"DeconvBlock.0.conv_transpose.weight\", \"DeconvBlock.0.conv_transpose.bias\", \"DeconvBlock.0.batchNorm.weight\", \"DeconvBlock.0.batchNorm.bias\", \"DeconvBlock.0.batchNorm.running_mean\", \"DeconvBlock.0.batchNorm.running_var\", \"DeconvBlock.0.batchNorm.num_batches_tracked\", \"DeconvBlock.2.conv_transpose.weight\", \"DeconvBlock.2.conv_transpose.bias\", \"DeconvBlock.2.batchNorm.weight\", \"DeconvBlock.2.batchNorm.bias\", \"DeconvBlock.2.batchNorm.running_mean\", \"DeconvBlock.2.batchNorm.running_var\", \"DeconvBlock.2.batchNorm.num_batches_tracked\", \"DeconvBlock.4.conv2d.weight\", \"DeconvBlock.4.conv2d.bias\", \"DeconvBlock.4.batchNorm.weight\", \"DeconvBlock.4.batchNorm.bias\", \"DeconvBlock.4.batchNorm.running_mean\", \"DeconvBlock.4.batchNorm.running_var\", \"DeconvBlock.4.batchNorm.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-168f527e9e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyleTransferNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#pth_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b9c68fa4b28b>\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(pth_path, model, optimizer)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for StyleTransferNetwork:\n\tMissing key(s) in state_dict: \"conv1.conv2d.weight\", \"conv1.conv2d.bias\", \"in1.weight\", \"in1.bias\", \"conv2.conv2d.weight\", \"conv2.conv2d.bias\", \"in2.weight\", \"in2.bias\", \"conv3.conv2d.weight\", \"conv3.conv2d.bias\", \"in3.weight\", \"in3.bias\", \"res1.conv1.conv2d.weight\", \"res1.conv1.conv2d.bias\", \"res1.in1.weight\", \"res1.in1.bias\", \"res1.conv2.conv2d.weight\", \"res1.conv2.conv2d.bias\", \"res1.in2.weight\", \"res1.in2.bias\", \"res2.conv1.conv2d.weight\", \"res2.conv1.conv2d.bias\", \"res2.in1.weight\", \"res2.in1.bias\", \"res2.conv2.conv2d.weight\", \"res2.conv2.conv2d.bias\", \"res2.in2.weight\", \"res2.in2.bias\", \"res3.conv1.conv2d.weight\", \"res3.conv1.conv2d.bias\", \"res3.in1.weight\", \"res3.in1.bias\", \"res3.conv2.conv2d.weight\", \"res3.conv2.conv2d.bias\", \"res3.in2.weight\", \"res3.in2.bias\", \"res4.conv1.conv2d.weight\", \"res4.conv1.conv2d.bias\", \"res4.in1.weight\", \"res4.in1.bias\", \"res4.conv2.conv2d.weight\", \"res4.conv2.conv2d.bias\", \"res4.in2.weight\", \"res4.in2.bias\", \"res5.conv1.conv2d.weight\", \"res5.conv1.conv2d.bias\", \"res5.in1.weight\", \"res5.in1.bias\", \"res5.conv2.conv2d.weight\", \"res5.conv2.conv2d.bias\", \"res5.in2.weight\", \"res5.in2.bias\", \"deconv1.conv2d.weight\", \"deconv1.conv2d.bias\", \"in4.weight\", \"in4.bias\", \"deconv2.conv2d.weight\", \"deconv2.conv2d.bias\", \"in5.weight\", \"in5.bias\", \"deconv3.conv2d.weight\", \"deconv3.conv2d.bias\". \n\tUnexpected key(s) in state_dict: \"ConvBlock.0.conv2d.weight\", \"ConvBlock.0.conv2d.bias\", \"ConvBlock.0.batchNorm.weight\", \"ConvBlock.0.batchNorm.bias\", \"ConvBlock.0.batchNorm.running_mean\", \"ConvBlock.0.batchNorm.running_var\", \"ConvBlock.0.batchNorm.num_batches_tracked\", \"ConvBlock.2.conv2d.weight\", \"ConvBlock.2.conv2d.bias\", \"ConvBlock.2.batchNorm.weight\", \"ConvBlock.2.batchNorm.bias\", \"ConvBlock.2.batchNorm.running_mean\", \"ConvBlock.2.batchNorm.running_var\", \"ConvBlock.2.batchNorm.num_batches_tracked\", \"ConvBlock.4.conv2d.weight\", \"ConvBlock.4.conv2d.bias\", \"ConvBlock.4.batchNorm.weight\", \"ConvBlock.4.batchNorm.bias\", \"ConvBlock.4.batchNorm.running_mean\", \"ConvBlock.4.batchNorm.running_var\", \"ConvBlock.4.batchNorm.num_batches_tracked\", \"ResBlock.0.conv1.conv2d.weight\", \"ResBlock.0.conv1.conv2d.bias\", \"ResBlock.0.conv1.batchNorm.weight\", \"ResBlock.0.conv1.batchNorm.bias\", \"ResBlock.0.conv1.batchNorm.running_mean\", \"ResBlock.0.conv1.batchNorm.running_var\", \"ResBlock.0.conv1.batchNorm.num_batches_tracked\", \"ResBlock.0.conv2.conv2d.weight\", \"ResBlock.0.conv2.conv2d.bias\", \"ResBlock.0.conv2.batchNorm.weight\", \"ResBlock.0.conv2.batchNorm.bias\", \"ResBlock.0.conv2.batchNorm.running_mean\", \"ResBlock.0.conv2.batchNorm.running_var\", \"ResBlock.0.conv2.batchNorm.num_batches_tracked\", \"ResBlock.1.conv1.conv2d.weight\", \"ResBlock.1.conv1.conv2d.bias\", \"ResBlock.1.conv1.batchNorm.weight\", \"ResBlock.1.conv1.batchNorm.bias\", \"ResBlock.1.conv1.batchNorm.running_mean\", \"ResBlock.1.conv1.batchNorm.running_var\", \"ResBlock.1.conv1.batchNorm.num_batches_tracked\", \"ResBlock.1.conv2.conv2d.weight\", \"ResBlock.1.conv2.conv2d.bias\", \"ResBlock.1.conv2.batchNorm.weight\", \"ResBlock.1.conv2.batchNorm.bias\", \"ResBlock.1.conv2.batchNorm.running_mean\", \"ResBlock.1.conv2.batchNorm.running_var\", \"ResBlock.1.conv2.batchNorm.num_batches_tracked\", \"ResBlock.2.conv1.conv2d.weight\", \"ResBlock.2.conv1.conv2d.bias\", \"ResBlock.2.conv1.batchNorm.weight\", \"ResBlock.2.conv1.batchNorm.bias\", \"ResBlock.2.conv1.batchNorm.running_mean\", \"ResBlock.2.conv1.batchNorm.running_var\", \"ResBlock.2.conv1.batchNorm.num_batches_tracked\", \"ResBlock.2.conv2.conv2d.weight\", \"ResBlock.2.conv2.conv2d.bias\", \"ResBlock.2.conv2.batchNorm.weight\", \"ResBlock.2.conv2.batchNorm.bias\", \"ResBlock.2.conv2.batchNorm.running_mean\", \"ResBlock.2.conv2.batchNorm.running_var\", \"ResBlock.2.conv2.batchNorm.num_batches_tracked\", \"ResBlock.3.conv1.conv2d.weight\", \"ResBlock.3.conv1.conv2d.bias\", \"ResBlock.3.conv1.batchNorm.weight\", \"ResBlock.3.conv1.batchNorm.bias\", \"ResBlock.3.conv1.batchNorm.running_mean\", \"ResBlock.3.conv1.batchNorm.running_var\", \"ResBlock.3.conv1.batchNorm.num_batches_tracked\", \"ResBlock.3.conv2.conv2d.weight\", \"ResBlock.3.conv2.conv2d.bias\", \"ResBlock.3.conv2.batchNorm.weight\", \"ResBlock.3.conv2.batchNorm.bias\", \"ResBlock.3.conv2.batchNorm.running_mean\", \"ResBlock.3.conv2.batchNorm.running_var\", \"ResBlock.3.conv2.batchNorm.num_batches_tracked\", \"ResBlock.4.conv1.conv2d.weight\", \"ResBlock.4.conv1.conv2d.bias\", \"ResBlock.4.conv1.batchNorm.weight\", \"ResBlock.4.conv1.batchNorm.bias\", \"ResBlock.4.conv1.batchNorm.running_mean\", \"ResBlock.4.conv1.batchNorm.running_var\", \"ResBlock.4.conv1.batchNorm.num_batches_tracked\", \"ResBlock.4.conv2.conv2d.weight\", \"ResBlock.4.conv2.conv2d.bias\", \"ResBlock.4.conv2.batchNorm.weight\", \"ResBlock.4.conv2.batchNorm.bias\", \"ResBlock.4.conv2.batchNorm.running_mean\", \"ResBlock.4.conv2.batchNorm.running_var\", \"ResBlock.4.conv2.batchNorm.num_batches_tracked\", \"DeconvBlock.0.conv_transpose.weight\", \"DeconvBlock.0.conv_transpose.bias\", \"DeconvBlock.0.batchNorm.weight\", \"DeconvBlock.0.batchNorm.bias\", \"DeconvBlock.0.batchNorm.running_mean\", \"DeconvBlock.0.batchNorm.running_var\", \"DeconvBlock.0.batchNorm.num_batches_tracked\", \"DeconvBlock.2.conv_transpose.weight\", \"DeconvBlock.2.conv_transpose.bias\", \"DeconvBlock.2.batchNorm.weight\", \"DeconvBlock.2.batchNorm.bias\", \"DeconvBlock.2.batchNorm.running_mean\", \"DeconvBlock.2.batchNorm.running_var\", \"DeconvBlock.2.batchNorm.num_batches_tracked\", \"DeconvBlock.4.conv2d.weight\", \"DeconvBlock.4.conv2d.bias\", \"DeconvBlock.4.batchNorm.weight\", \"DeconvBlock.4.batchNorm.bias\", \"DeconvBlock.4.batchNorm.running_mean\", \"DeconvBlock.4.batchNorm.running_var\", \"DeconvBlock.4.batchNorm.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "# TODO: Adapt the path so that it points to your model\n",
    "pth_path = os.path.join('checkpoints', '00012_checkpoint_mosaic_content_weight_5.0_style_weight_100.0_tv_weight_1e-06', '004000.pth')\n",
    "model = StyleTransferNetwork().to(device)\n",
    "#pth_path\n",
    "load_checkpoint(pth_path, model)\n",
    "\n",
    "\n",
    "output_path = \"output\"\n",
    "content_torch = load_image(content_img_path).to(device)\n",
    "out = model(content_torch)\n",
    "result_to_hd(out[0], os.path.join(output_path, content_img_name + '_' + style_names[style_idx] + '.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
